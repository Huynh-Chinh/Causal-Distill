{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Training the distilled model.\n",
    "Supported architectures include: BERT -> DistilBERT, RoBERTa -> DistilRoBERTa, GPT2 -> DistilGPT2.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from causal_distiller import *\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "from models.modeling_distilbert import DistilBertForMaskedLM # we need to customize it a little.\n",
    "from utils import git_log, init_gpu_params, logger, set_seed\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_checks(args):\n",
    "    \"\"\"\n",
    "    A bunch of args sanity checks to perform even starting...\n",
    "    \"\"\"\n",
    "    assert (args.mlm and args.alpha_mlm > 0.0) or (not args.mlm and args.alpha_mlm == 0.0)\n",
    "    assert (args.alpha_mlm > 0.0 and args.alpha_clm == 0.0) or (args.alpha_mlm == 0.0 and args.alpha_clm > 0.0)\n",
    "    if args.mlm:\n",
    "        assert os.path.isfile(args.token_counts)\n",
    "        assert (args.student_type in [\"roberta\", \"distilbert\"]) and (args.teacher_type in [\"roberta\", \"bert\"])\n",
    "    else:\n",
    "        assert (args.student_type in [\"gpt2\"]) and (args.teacher_type in [\"gpt2\"])\n",
    "\n",
    "    assert args.teacher_type == args.student_type or (\n",
    "        args.student_type == \"distilbert\" and args.teacher_type == \"bert\"\n",
    "    )\n",
    "    assert os.path.isfile(args.student_config)\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        assert os.path.isfile(args.student_pretrained_weights)\n",
    "\n",
    "    if args.freeze_token_type_embds:\n",
    "        assert args.student_type in [\"roberta\"]\n",
    "\n",
    "    assert args.alpha_ce >= 0.0\n",
    "    assert args.alpha_mlm >= 0.0\n",
    "    assert args.alpha_clm >= 0.0\n",
    "    assert args.alpha_mse >= 0.0\n",
    "    assert args.alpha_cos >= 0.0\n",
    "    assert args.alpha_causal >= 0.0\n",
    "    assert args.alpha_ce + args.alpha_mlm + args.alpha_clm + args.alpha_mse + args.alpha_cos + args.alpha_causal > 0.0\n",
    "\n",
    "\n",
    "def freeze_pos_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "    elif args.student_type == \"gpt2\":\n",
    "        student.transformer.wpe.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def freeze_token_type_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def prepare_distiller(args):\n",
    "\n",
    "    # ARGS #\n",
    "    init_gpu_params(args)\n",
    "    set_seed(args)\n",
    "    if args.is_master:\n",
    "        if os.path.exists(args.dump_path):\n",
    "            if not args.force:\n",
    "                raise ValueError(\n",
    "                    f\"Serialization dir {args.dump_path} already exists, but you have not precised wheter to overwrite it\"\n",
    "                    \"Use `--force` if you want to overwrite it\"\n",
    "                )\n",
    "            else:\n",
    "                shutil.rmtree(args.dump_path)\n",
    "\n",
    "        if not os.path.exists(args.dump_path):\n",
    "            os.makedirs(args.dump_path)\n",
    "        logger.info(f\"Experiment will be dumped and logged in {args.dump_path}\")\n",
    "\n",
    "        # SAVE PARAMS #\n",
    "        logger.info(f\"Param: {args}\")\n",
    "        with open(os.path.join(args.dump_path, \"parameters.json\"), \"w\") as f:\n",
    "            json.dump(vars(args), f, indent=4)\n",
    "        git_log(args.dump_path)\n",
    "\n",
    "    student_config_class, student_model_class, _ = MODEL_CLASSES[args.student_type]\n",
    "    teacher_config_class, teacher_model_class, teacher_tokenizer_class = MODEL_CLASSES[args.teacher_type]\n",
    "\n",
    "    # TOKENIZER #\n",
    "    tokenizer = teacher_tokenizer_class.from_pretrained(\n",
    "        args.teacher_name,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    special_tok_ids = {}\n",
    "    for tok_name, tok_symbol in tokenizer.special_tokens_map.items():\n",
    "        idx = tokenizer.all_special_tokens.index(tok_symbol)\n",
    "        special_tok_ids[tok_name] = tokenizer.all_special_ids[idx]\n",
    "    logger.info(f\"Special tokens {special_tok_ids}\")\n",
    "    args.special_tok_ids = special_tok_ids\n",
    "    args.max_model_input_size = tokenizer.max_model_input_sizes[args.teacher_name]\n",
    "\n",
    "    # DATA LOADER #\n",
    "    logger.info(f\"Loading data from {args.data_file}\")\n",
    "    with open(args.data_file, \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    if args.mlm:\n",
    "        logger.info(f\"Loading token counts from {args.token_counts} (already pre-computed)\")\n",
    "        with open(args.token_counts, \"rb\") as fp:\n",
    "            counts = pickle.load(fp)\n",
    "\n",
    "        token_probs = np.maximum(counts, 1) ** -args.mlm_smoothing\n",
    "        for idx in special_tok_ids.values():\n",
    "            token_probs[idx] = 0.0  # do not predict special tokens\n",
    "        token_probs = torch.from_numpy(token_probs)\n",
    "    else:\n",
    "        token_probs = None\n",
    "\n",
    "    train_lm_seq_dataset = LmSeqsDataset(params=args, data=data)\n",
    "    logger.info(\"Data loader created.\")\n",
    "\n",
    "    # STUDENT #\n",
    "    logger.info(f\"Loading student config from {args.student_config}\")\n",
    "    stu_architecture_config = student_config_class.from_pretrained(\n",
    "        args.student_config,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    stu_architecture_config.output_hidden_states = True\n",
    "\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        logger.info(f\"Loading pretrained weights from {args.student_pretrained_weights}\")\n",
    "        student = student_model_class.from_pretrained(\n",
    "            args.student_pretrained_weights, config=stu_architecture_config,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "    else:\n",
    "        student = student_model_class(stu_architecture_config)\n",
    "\n",
    "    if args.n_gpu > 0:\n",
    "        student.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "    logger.info(\"Student loaded.\")\n",
    "\n",
    "    # TEACHER #\n",
    "    teacher = teacher_model_class.from_pretrained(\n",
    "        args.teacher_name, output_hidden_states=True, \n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    if args.n_gpu > 0:\n",
    "        teacher.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "    logger.info(f\"Teacher loaded from {args.teacher_name}.\")\n",
    "\n",
    "    # FREEZING #\n",
    "    if args.freeze_pos_embs:\n",
    "        freeze_pos_embeddings(student, args)\n",
    "    if args.freeze_token_type_embds:\n",
    "        freeze_token_type_embeddings(student, args)\n",
    "\n",
    "    # SANITY CHECKS #\n",
    "    assert student.config.vocab_size == teacher.config.vocab_size\n",
    "    assert student.config.hidden_size == teacher.config.hidden_size\n",
    "    assert student.config.max_position_embeddings == teacher.config.max_position_embeddings\n",
    "    if args.mlm:\n",
    "        assert token_probs.size(0) == stu_architecture_config.vocab_size\n",
    "\n",
    "    # DISTILLER #\n",
    "    torch.cuda.empty_cache()\n",
    "    distiller = CausalDistiller(\n",
    "        params=args, dataset=train_lm_seq_dataset, token_probs=token_probs, student=student, teacher=teacher\n",
    "    )\n",
    "    logger.info(\"Distiller initialization done.\")\n",
    "    return distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2021 21:16:21 - INFO - utils - PID: 23668 -  Experiment will be dumped and logged in ./results/s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25\n",
      "10/16/2021 21:16:21 - INFO - utils - PID: 23668 -  Param: Namespace(adam_epsilon=1e-06, alpha_causal=0.25, alpha_ce=0.25, alpha_clm=0.0, alpha_cos=0.25, alpha_mlm=0.25, alpha_mse=0.0, batch_size=5, cache_dir='./distill_cache/', checkpoint_interval=4000, data_file='./demo_data/binarized_text.train.bert-base-uncased.pickle', dump_path='./results/s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25', force=True, fp16=False, fp16_opt_level='O1', freeze_pos_embs=False, freeze_token_type_embds=False, gradient_accumulation_steps=50, group_by_size=True, initializer_range=0.02, interchange_mlm=False, interchange_prop=0.3, is_master=True, is_wandb=False, learning_rate=0.0005, local_rank=-1, log_interval=10, master_port=-1, max_grad_norm=5.0, mlm=True, mlm_mask_prop=0.15, mlm_smoothing=0.7, multi_gpu=False, n_epoch=3, n_gpu=0, neuron_mapping='./training_configs/single_middle.nm', restrict_ce_to_mask=False, run_name='s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25', seed=56, student_config='./training_configs/distilbert-base-uncased.json', student_pretrained_weights=None, student_type='distilbert', teacher_name='bert-base-uncased', teacher_type='bert', temperature=2.0, token_counts='./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle', warmup_prop=0.05, weight_decay=0.0, word_keep=0.1, word_mask=0.8, word_rand=0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prelude: running in notebook for testing only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Special tokens {'unk_token': 100, 'sep_token': 102, 'pad_token': 0, 'cls_token': 101, 'mask_token': 103}\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Loading data from ./demo_data/binarized_text.train.bert-base-uncased.pickle\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Loading token counts from ./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle (already pre-computed)\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Splitting 0 too long sequences.\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Remove 542 too short (<=11 tokens) sequences.\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Remove 0 sequences with a high level of unknown tokens (50%).\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Preparing causal batch.\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  458 sequences\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Data loader created.\n",
      "10/16/2021 21:16:23 - INFO - utils - PID: 23668 -  Loading student config from ./training_configs/distilbert-base-uncased.json\n",
      "10/16/2021 21:16:25 - INFO - utils - PID: 23668 -  Student loaded.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Teacher loaded from bert-base-uncased.\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Initializing Distiller\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Neuron Mapping: {'interchange_variable_mappings': [{'teacher_variable_names': ['$L:6$H:[0:12]$[0:64]'], 'student_variable_names': ['$L:1$H:[0:12]$[0:64]']}]}\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Using [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499, 503, 507, 511, inf] as bins for aspect lengths quantization\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Count of instances per bin: [54 28 12  7  3  1  1  3  4  3  2  3  4  2  9  6  3  5 13 11  9 11  4  6\n",
      "  5  9  7  8 12  5  7  2  5  9  5  9 11  7  8  8  3  2  3  5  7  6  6  3\n",
      "  7  7  6  2  1  2  8  4  5  3  1  3  4  3  6  3  1  1  2  1  2  2  2  4\n",
      "  1  1  4  1  1  2  2  1  1  1  1  3  1  1  1]\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Using MLM loss for LM step.\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  --- Initializing model optimizer\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  ------ Number of trainable parameters (student): 66592314\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  ------ Number of parameters (student): 66985530\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Distiller initialization done.\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  Starting training\n",
      "10/16/2021 21:16:28 - INFO - utils - PID: 23668 -  --- Starting epoch 0/2\n",
      "-Iter: 100%|██████████| 92/92 [04:39<00:00,  3.04s/it, Last_loss=15.57, Avg_cum_loss=14.45, Last_cf_loss=25.41]\n",
      "10/16/2021 21:21:07 - INFO - utils - PID: 23668 -  --- Ending epoch 0/2\n",
      "10/16/2021 21:21:07 - INFO - utils - PID: 23668 -  458 sequences have been trained during this epoch.\n",
      "10/16/2021 21:21:08 - INFO - utils - PID: 23668 -  --- Starting epoch 1/2\n",
      "-Iter: 100%|██████████| 92/92 [04:36<00:00,  3.00s/it, Last_loss=14.11, Avg_cum_loss=13.82, Last_cf_loss=22.74]\n",
      "10/16/2021 21:25:44 - INFO - utils - PID: 23668 -  --- Ending epoch 1/2\n",
      "10/16/2021 21:25:44 - INFO - utils - PID: 23668 -  458 sequences have been trained during this epoch.\n",
      "10/16/2021 21:25:45 - INFO - utils - PID: 23668 -  --- Starting epoch 2/2\n",
      "-Iter: 100%|██████████| 92/92 [04:38<00:00,  3.03s/it, Last_loss=13.26, Avg_cum_loss=12.79, Last_cf_loss=21.08]\n",
      "10/16/2021 21:30:24 - INFO - utils - PID: 23668 -  --- Ending epoch 2/2\n",
      "10/16/2021 21:30:24 - INFO - utils - PID: 23668 -  458 sequences have been trained during this epoch.\n",
      "10/16/2021 21:30:25 - INFO - utils - PID: 23668 -  Save very last checkpoint as `pytorch_model.bin`.\n",
      "10/16/2021 21:30:25 - INFO - utils - PID: 23668 -  Training is finished\n",
      "10/16/2021 21:30:25 - INFO - utils - PID: 23668 -  Hey Zen: Let's go get some drinks.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Training\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite dump_path if it already exists.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dump_path\", type=str, help=\"The output directory (log, checkpoints, parameters, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_file\",\n",
    "        type=str,\n",
    "        help=\"The binarized file (tokenized + tokens_to_ids) and grouped by sequence.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        type=str,\n",
    "        help=\"You need to give some cache dir.\",\n",
    "        default=\"./distill_cache/\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--student_type\",\n",
    "        type=str,\n",
    "        choices=[\"distilbert\", \"roberta\", \"gpt2\"],\n",
    "        help=\"The student type (DistilBERT, RoBERTa).\",\n",
    "    )\n",
    "    parser.add_argument(\"--student_config\", type=str, help=\"Path to the student configuration.\")\n",
    "    parser.add_argument(\n",
    "        \"--student_pretrained_weights\", default=None, type=str, help=\"Load student initialization checkpoint.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--teacher_type\", choices=[\"bert\", \"roberta\", \"gpt2\"], help=\"Teacher type (BERT, RoBERTa).\"\n",
    "    )\n",
    "    parser.add_argument(\"--teacher_name\", type=str, help=\"The teacher model.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--neuron_mapping\",\n",
    "        type=str,\n",
    "        help=\"Predefined neuron mapping for the interchange experiment.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--temperature\", default=2.0, type=float, help=\"Temperature for the softmax temperature.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_ce\", default=0.5, type=float, help=\"Linear weight for the distillation loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_mlm\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=\"Linear weight for the MLM loss. Must be >=0. Should be used in coonjunction with `mlm` flag.\",\n",
    "    )\n",
    "    parser.add_argument(\"--alpha_clm\", default=0.5, type=float, help=\"Linear weight for the CLM loss. Must be >=0.\")\n",
    "    parser.add_argument(\"--alpha_mse\", default=0.0, type=float, help=\"Linear weight of the MSE loss. Must be >=0.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_cos\", default=0.0, type=float, help=\"Linear weight of the cosine embedding loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_causal\", default=0.0, type=float, help=\"Linear weight of the causal distillation loss. Must be >=0.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--mlm\", action=\"store_true\", help=\"The LM step: MLM or CLM. If `mlm` is True, the MLM is used over CLM.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mlm_mask_prop\",\n",
    "        default=0.15,\n",
    "        type=float,\n",
    "        help=\"Proportion of tokens for which we need to make a prediction.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--interchange_mlm\", action=\"store_true\", help=\"Whehter to follow mlm to select token positions to do interchange.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--interchange_prop\",\n",
    "        default=0.3,\n",
    "        type=float,\n",
    "        help=\"Ratio of tokens to mask for interchange interventions. 1.0 means interchange all.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--word_mask\", default=0.8, type=float, help=\"Proportion of tokens to mask out.\")\n",
    "    parser.add_argument(\"--word_keep\", default=0.1, type=float, help=\"Proportion of tokens to keep.\")\n",
    "    parser.add_argument(\"--word_rand\", default=0.1, type=float, help=\"Proportion of tokens to randomly replace.\")\n",
    "    parser.add_argument(\n",
    "        \"--mlm_smoothing\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "        help=\"Smoothing parameter to emphasize more rare tokens (see XLM, similar to word2vec).\",\n",
    "    )\n",
    "    parser.add_argument(\"--token_counts\", type=str, help=\"The token counts in the data_file for MLM.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--restrict_ce_to_mask\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If true, compute the distilation loss only the [MLM] prediction distribution.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_pos_embs\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze positional embeddings during distillation. For student_type in ['roberta', 'gpt2'] only.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_token_type_embds\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze token type embeddings during distillation if existent. For student_type in ['roberta'] only.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--n_epoch\", type=int, default=3, help=\"Number of pass on the whole dataset.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=5, help=\"Batch size (for each process).\")\n",
    "    parser.add_argument(\n",
    "        \"--group_by_size\",\n",
    "        action=\"store_false\",\n",
    "        help=\"If true, group sequences that have similar length into the same batch. Default is true.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Gradient accumulation for larger training batches.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_prop\", default=0.05, type=float, help=\"Linear warmup proportion.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-4, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=5.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--initializer_range\", default=0.02, type=float, help=\"Random initialization range.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--n_gpu\", type=int, default=1, help=\"Number of GPUs in the node.\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Distributed training - Local rank\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=56, help=\"Random seed\")\n",
    "\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=500, help=\"Tensorboard logging interval.\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=4000, help=\"Checkpoint interval.\")\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--is_wandb\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If true, we will log everything to wandb that is logged in currently.\",\n",
    "    )\n",
    "    parser.add_argument(\"--run_name\", type=str, help=\"Name of this run.\")\n",
    "    \n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        parser.set_defaults(\n",
    "            # Exp management:\n",
    "            student_type=\"distilbert\",\n",
    "            teacher_type=\"bert\",\n",
    "            mlm=True,\n",
    "            alpha_ce=0.25,\n",
    "            alpha_mlm=0.25,\n",
    "            alpha_cos=0.25,\n",
    "            alpha_clm=0.0,\n",
    "            alpha_causal=0.25,\n",
    "            token_counts=\"./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle\",\n",
    "            student_config=\"./training_configs/distilbert-base-uncased.json\",\n",
    "            dump_path=\"./results/\",\n",
    "            teacher_name=\"bert-base-uncased\",\n",
    "            force=True,\n",
    "            data_file=\"./demo_data/binarized_text.train.bert-base-uncased.pickle\",\n",
    "            n_gpu=0,\n",
    "            is_wandb=False,\n",
    "            log_interval=10,\n",
    "            neuron_mapping=\"./training_configs/single_middle.nm\",\n",
    "            local_rank=-1,\n",
    "            interchange_prop=0.3\n",
    "        )\n",
    "        print(\"Prelude: running in notebook for testing only.\")\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        print(\"Prelude: running with command line.\")\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    # config the runname here and overwrite.\n",
    "    data_name = args.data_file.split(\"/\")[-2]\n",
    "    run_name = f\"s_{args.student_type}_t_{args.teacher_type}_data_{data_name}_seed_{args.seed}_mlm_{args.mlm}_ce_{args.alpha_ce}_mlm_{args.alpha_mlm}_cos_{args.alpha_cos}_causal_{args.alpha_causal}\"\n",
    "    args.run_name = run_name\n",
    "    args.dump_path = os.path.join(args.dump_path, args.run_name)\n",
    "    sanity_checks(args)\n",
    "    \n",
    "    distiller = prepare_distiller(args)\n",
    "    \n",
    "    distiller.train()\n",
    "    logger.info(\"Hey Zen: Let's go get some drinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
