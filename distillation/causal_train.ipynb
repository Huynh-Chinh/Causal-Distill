{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Training the distilled model.\n",
    "Supported architectures include: BERT -> DistilBERT, RoBERTa -> DistilRoBERTa, GPT2 -> DistilGPT2.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from causal_distiller import *\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "from models.modeling_distilbert import DistilBertForMaskedLM # we need to customize it a little.\n",
    "from utils import git_log, init_gpu_params, logger, set_seed\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_checks(args):\n",
    "    \"\"\"\n",
    "    A bunch of args sanity checks to perform even starting...\n",
    "    \"\"\"\n",
    "    assert (args.mlm and args.alpha_mlm > 0.0) or (not args.mlm and args.alpha_mlm == 0.0)\n",
    "    assert (args.alpha_mlm > 0.0 and args.alpha_clm == 0.0) or (args.alpha_mlm == 0.0 and args.alpha_clm > 0.0)\n",
    "    if args.mlm:\n",
    "        assert os.path.isfile(args.token_counts)\n",
    "        assert (args.student_type in [\"roberta\", \"distilbert\"]) and (args.teacher_type in [\"roberta\", \"bert\"])\n",
    "    else:\n",
    "        assert (args.student_type in [\"gpt2\"]) and (args.teacher_type in [\"gpt2\"])\n",
    "\n",
    "    assert args.teacher_type == args.student_type or (\n",
    "        args.student_type == \"distilbert\" and args.teacher_type == \"bert\"\n",
    "    )\n",
    "    assert os.path.isfile(args.student_config)\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        assert os.path.isfile(args.student_pretrained_weights)\n",
    "\n",
    "    if args.freeze_token_type_embds:\n",
    "        assert args.student_type in [\"roberta\"]\n",
    "\n",
    "    assert args.alpha_ce >= 0.0\n",
    "    assert args.alpha_mlm >= 0.0\n",
    "    assert args.alpha_clm >= 0.0\n",
    "    assert args.alpha_mse >= 0.0\n",
    "    assert args.alpha_cos >= 0.0\n",
    "    assert args.alpha_causal >= 0.0\n",
    "    assert args.alpha_ce + args.alpha_mlm + args.alpha_clm + args.alpha_mse + args.alpha_cos + args.alpha_causal > 0.0\n",
    "\n",
    "\n",
    "def freeze_pos_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "    elif args.student_type == \"gpt2\":\n",
    "        student.transformer.wpe.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def freeze_token_type_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def prepare_distiller(args):\n",
    "\n",
    "    # ARGS #\n",
    "    init_gpu_params(args)\n",
    "    set_seed(args)\n",
    "    if args.is_master:\n",
    "        if os.path.exists(args.dump_path):\n",
    "            if not args.force:\n",
    "                raise ValueError(\n",
    "                    f\"Serialization dir {args.dump_path} already exists, but you have not precised wheter to overwrite it\"\n",
    "                    \"Use `--force` if you want to overwrite it\"\n",
    "                )\n",
    "            else:\n",
    "                shutil.rmtree(args.dump_path)\n",
    "\n",
    "        if not os.path.exists(args.dump_path):\n",
    "            os.makedirs(args.dump_path)\n",
    "        logger.info(f\"Experiment will be dumped and logged in {args.dump_path}\")\n",
    "\n",
    "        # SAVE PARAMS #\n",
    "        logger.info(f\"Param: {args}\")\n",
    "        with open(os.path.join(args.dump_path, \"parameters.json\"), \"w\") as f:\n",
    "            json.dump(vars(args), f, indent=4)\n",
    "        git_log(args.dump_path)\n",
    "\n",
    "    student_config_class, student_model_class, _ = MODEL_CLASSES[args.student_type]\n",
    "    teacher_config_class, teacher_model_class, teacher_tokenizer_class = MODEL_CLASSES[args.teacher_type]\n",
    "\n",
    "    # TOKENIZER #\n",
    "    tokenizer = teacher_tokenizer_class.from_pretrained(\n",
    "        args.teacher_name,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    special_tok_ids = {}\n",
    "    for tok_name, tok_symbol in tokenizer.special_tokens_map.items():\n",
    "        idx = tokenizer.all_special_tokens.index(tok_symbol)\n",
    "        special_tok_ids[tok_name] = tokenizer.all_special_ids[idx]\n",
    "    logger.info(f\"Special tokens {special_tok_ids}\")\n",
    "    args.special_tok_ids = special_tok_ids\n",
    "    args.max_model_input_size = tokenizer.max_model_input_sizes[args.teacher_name]\n",
    "\n",
    "    # DATA LOADER #\n",
    "    logger.info(f\"Loading data from {args.data_file}\")\n",
    "    with open(args.data_file, \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    if args.mlm:\n",
    "        logger.info(f\"Loading token counts from {args.token_counts} (already pre-computed)\")\n",
    "        with open(args.token_counts, \"rb\") as fp:\n",
    "            counts = pickle.load(fp)\n",
    "\n",
    "        token_probs = np.maximum(counts, 1) ** -args.mlm_smoothing\n",
    "        for idx in special_tok_ids.values():\n",
    "            token_probs[idx] = 0.0  # do not predict special tokens\n",
    "        token_probs = torch.from_numpy(token_probs)\n",
    "    else:\n",
    "        token_probs = None\n",
    "\n",
    "    train_lm_seq_dataset = LmSeqsDataset(params=args, data=data)\n",
    "    logger.info(\"Data loader created.\")\n",
    "\n",
    "    # STUDENT #\n",
    "    logger.info(f\"Loading student config from {args.student_config}\")\n",
    "    stu_architecture_config = student_config_class.from_pretrained(\n",
    "        args.student_config,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    stu_architecture_config.output_hidden_states = True\n",
    "\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        logger.info(f\"Loading pretrained weights from {args.student_pretrained_weights}\")\n",
    "        student = student_model_class.from_pretrained(\n",
    "            args.student_pretrained_weights, config=stu_architecture_config,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "    else:\n",
    "        student = student_model_class(stu_architecture_config)\n",
    "\n",
    "    if args.n_gpu > 0:\n",
    "        student.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "    logger.info(\"Student loaded.\")\n",
    "\n",
    "    # TEACHER #\n",
    "    teacher = teacher_model_class.from_pretrained(\n",
    "        args.teacher_name, output_hidden_states=True, \n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    if args.n_gpu > 0:\n",
    "        teacher.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "    logger.info(f\"Teacher loaded from {args.teacher_name}.\")\n",
    "\n",
    "    # FREEZING #\n",
    "    if args.freeze_pos_embs:\n",
    "        freeze_pos_embeddings(student, args)\n",
    "    if args.freeze_token_type_embds:\n",
    "        freeze_token_type_embeddings(student, args)\n",
    "\n",
    "    # SANITY CHECKS #\n",
    "    assert student.config.vocab_size == teacher.config.vocab_size\n",
    "    assert student.config.hidden_size == teacher.config.hidden_size\n",
    "    assert student.config.max_position_embeddings == teacher.config.max_position_embeddings\n",
    "    if args.mlm:\n",
    "        assert token_probs.size(0) == stu_architecture_config.vocab_size\n",
    "\n",
    "    # DISTILLER #\n",
    "    torch.cuda.empty_cache()\n",
    "    distiller = CausalDistiller(\n",
    "        params=args, dataset=train_lm_seq_dataset, token_probs=token_probs, student=student, teacher=teacher\n",
    "    )\n",
    "    logger.info(\"Distiller initialization done.\")\n",
    "    return distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Training\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite dump_path if it already exists.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dump_path\", type=str, help=\"The output directory (log, checkpoints, parameters, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_file\",\n",
    "        type=str,\n",
    "        help=\"The binarized file (tokenized + tokens_to_ids) and grouped by sequence.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        type=str,\n",
    "        help=\"You need to give some cache dir.\",\n",
    "        default=\"./distill_cache/\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--student_type\",\n",
    "        type=str,\n",
    "        choices=[\"distilbert\", \"roberta\", \"gpt2\"],\n",
    "        help=\"The student type (DistilBERT, RoBERTa).\",\n",
    "    )\n",
    "    parser.add_argument(\"--student_config\", type=str, help=\"Path to the student configuration.\")\n",
    "    parser.add_argument(\n",
    "        \"--student_pretrained_weights\", default=None, type=str, help=\"Load student initialization checkpoint.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--teacher_type\", choices=[\"bert\", \"roberta\", \"gpt2\"], help=\"Teacher type (BERT, RoBERTa).\"\n",
    "    )\n",
    "    parser.add_argument(\"--teacher_name\", type=str, help=\"The teacher model.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--neuron_mapping\",\n",
    "        type=str,\n",
    "        help=\"Predefined neuron mapping for the interchange experiment.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--temperature\", default=2.0, type=float, help=\"Temperature for the softmax temperature.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_ce\", default=0.5, type=float, help=\"Linear weight for the distillation loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_mlm\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=\"Linear weight for the MLM loss. Must be >=0. Should be used in coonjunction with `mlm` flag.\",\n",
    "    )\n",
    "    parser.add_argument(\"--alpha_clm\", default=0.5, type=float, help=\"Linear weight for the CLM loss. Must be >=0.\")\n",
    "    parser.add_argument(\"--alpha_mse\", default=0.0, type=float, help=\"Linear weight of the MSE loss. Must be >=0.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_cos\", default=0.0, type=float, help=\"Linear weight of the cosine embedding loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_causal\", default=0.0, type=float, help=\"Linear weight of the causal distillation loss. Must be >=0.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--mlm\", action=\"store_true\", help=\"The LM step: MLM or CLM. If `mlm` is True, the MLM is used over CLM.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mlm_mask_prop\",\n",
    "        default=0.15,\n",
    "        type=float,\n",
    "        help=\"Proportion of tokens for which we need to make a prediction.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--interchange_mlm\", action=\"store_true\", help=\"Whehter to follow mlm to select token positions to do interchange.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--interchange_prop\",\n",
    "        default=0.3,\n",
    "        type=float,\n",
    "        help=\"Ratio of tokens to mask for interchange interventions. 1.0 means interchange all.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--word_mask\", default=0.8, type=float, help=\"Proportion of tokens to mask out.\")\n",
    "    parser.add_argument(\"--word_keep\", default=0.1, type=float, help=\"Proportion of tokens to keep.\")\n",
    "    parser.add_argument(\"--word_rand\", default=0.1, type=float, help=\"Proportion of tokens to randomly replace.\")\n",
    "    parser.add_argument(\n",
    "        \"--mlm_smoothing\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "        help=\"Smoothing parameter to emphasize more rare tokens (see XLM, similar to word2vec).\",\n",
    "    )\n",
    "    parser.add_argument(\"--token_counts\", type=str, help=\"The token counts in the data_file for MLM.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--restrict_ce_to_mask\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If true, compute the distilation loss only the [MLM] prediction distribution.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_pos_embs\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze positional embeddings during distillation. For student_type in ['roberta', 'gpt2'] only.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_token_type_embds\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze token type embeddings during distillation if existent. For student_type in ['roberta'] only.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--n_epoch\", type=int, default=3, help=\"Number of pass on the whole dataset.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=5, help=\"Batch size (for each process).\")\n",
    "    parser.add_argument(\n",
    "        \"--group_by_size\",\n",
    "        action=\"store_false\",\n",
    "        help=\"If true, group sequences that have similar length into the same batch. Default is true.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Gradient accumulation for larger training batches.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_prop\", default=0.05, type=float, help=\"Linear warmup proportion.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-4, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=5.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--initializer_range\", default=0.02, type=float, help=\"Random initialization range.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--n_gpu\", type=int, default=1, help=\"Number of GPUs in the node.\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Distributed training - Local rank\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=56, help=\"Random seed\")\n",
    "\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=500, help=\"Tensorboard logging interval.\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=4000, help=\"Checkpoint interval.\")\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--is_wandb\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If true, we will log everything to wandb that is logged in currently.\",\n",
    "    )\n",
    "    parser.add_argument(\"--run_name\", type=str, help=\"Name of this run.\")\n",
    "    \n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        parser.set_defaults(\n",
    "            # Exp management:\n",
    "            student_type=\"distilbert\",\n",
    "            teacher_type=\"bert\",\n",
    "            mlm=True,\n",
    "            alpha_ce=0.25,\n",
    "            alpha_mlm=0.25,\n",
    "            alpha_cos=0.25,\n",
    "            alpha_clm=0.0,\n",
    "            alpha_causal=0.25,\n",
    "            token_counts=\"./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle\",\n",
    "            student_config=\"./training_configs/distilbert-base-uncased.json\",\n",
    "            dump_path=\"./results/\",\n",
    "            teacher_name=\"bert-base-uncased\",\n",
    "            force=True,\n",
    "            data_file=\"./demo_data/binarized_text.train.bert-base-uncased.pickle\",\n",
    "            n_gpu=0,\n",
    "            is_wandb=False,\n",
    "            log_interval=10,\n",
    "            neuron_mapping=\"./training_configs/neuron_mapping.json\",\n",
    "            local_rank=-1,\n",
    "            interchange_prop=0.3\n",
    "        )\n",
    "        print(\"Prelude: running in notebook for testing only.\")\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        print(\"Prelude: running with command line.\")\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    # config the runname here and overwrite.\n",
    "    data_name = args.data_file.split(\"/\")[-2]\n",
    "    run_name = f\"s_{args.student_type}_t_{args.teacher_type}_data_{data_name}_seed_{args.seed}_mlm_{args.mlm}_ce_{args.alpha_ce}_mlm_{args.alpha_mlm}_cos_{args.alpha_cos}_causal_{args.alpha_causal}\"\n",
    "    args.run_name = run_name\n",
    "    args.dump_path = os.path.join(args.dump_path, args.run_name)\n",
    "    sanity_checks(args)\n",
    "    \n",
    "    distiller = prepare_distiller(args)\n",
    "    \n",
    "    distiller.train()\n",
    "    logger.info(\"Hey Zen: Let's go get some drinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
