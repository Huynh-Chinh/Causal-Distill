{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Training the distilled model.\n",
    "Supported architectures include: BERT -> DistilBERT, RoBERTa -> DistilRoBERTa, GPT2 -> DistilGPT2.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from causal_distiller import *\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "from models.modeling_distilbert import DistilBertForMaskedLM # we need to customize it a little.\n",
    "from models.modeling_bert import BertForMaskedLM # we need to customize it a little.\n",
    "from utils import git_log, init_gpu_params, logger, set_seed\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" The distiller to distil the student.\n",
    "    Adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM)\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from grouped_batch_sampler import GroupedBatchSampler, create_lengths_groups\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from utils import logger\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from distiller import Distiller\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    ")\n",
    "from utils import git_log, init_gpu_params, logger, set_seed\n",
    "from datasets import load_dataset\n",
    "from counterfactual_utils import *\n",
    "import wandb\n",
    "from models.modeling_distilbert import DistilBertForMaskedLM\n",
    "\n",
    "# Examples of interchange.\n",
    "# activations_counterfactual_teacher = get_activation_at(\n",
    "#     teacher_bert,\n",
    "#     batch[\"input_ids\"],\n",
    "#     batch[\"attention_mask\"],\n",
    "#     variable_names=[\"$L:1$H:1$[0:32]\"]\n",
    "# )\n",
    "# interchange_with_activation_at(\n",
    "#     teacher_bert,\n",
    "#     batch[\"input_ids\"],\n",
    "#     batch[\"attention_mask\"],\n",
    "#     interchanged_variables=[torch.zeros(32, 512, 32)],\n",
    "#     variable_names=[\"$L:1$H:1$[0:32]\"]\n",
    "# )\n",
    "\n",
    "class CausalDistiller:\n",
    "    def __init__(\n",
    "        self, params: dict, dataset: LmSeqsDataset, \n",
    "        token_probs: torch.tensor, student: nn.Module, teacher: nn.Module\n",
    "    ):\n",
    "        if params.is_wandb:\n",
    "            run = wandb.init(\n",
    "                project=\"Causal-BERT-Distillation\", \n",
    "                entity=\"wuzhengx\",\n",
    "                name=params.run_name,\n",
    "            )\n",
    "            wandb.config.update(params)\n",
    "        self.is_wandb = params.is_wandb\n",
    "        \n",
    "        logger.info(\"Initializing Distiller\")\n",
    "        self.params = params\n",
    "        self.dump_path = params.dump_path\n",
    "        self.multi_gpu = params.multi_gpu\n",
    "        self.fp16 = params.fp16\n",
    "\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        \n",
    "        # causal neuron mappings.\n",
    "        self.deserialized_interchange_variable_mappings = []\n",
    "        with open(params.neuron_mapping) as json_file:\n",
    "            neuron_mapping = json.load(json_file)\n",
    "            logger.info(f\"Neuron Mapping: {neuron_mapping}\")\n",
    "            interchange_variable_mappings = neuron_mapping[\"interchange_variable_mappings\"]\n",
    "            for m in interchange_variable_mappings:\n",
    "                teacher_deserialized_variables = []\n",
    "                for variable in m[\"teacher_variable_names\"]:\n",
    "                    teacher_deserialized_variables.append(deserialize_variable_name(variable))\n",
    "                student_deserialized_variables = []\n",
    "                for variable in m[\"student_variable_names\"]:\n",
    "                    student_deserialized_variables.append(deserialize_variable_name(variable))\n",
    "                self.deserialized_interchange_variable_mappings += [\n",
    "                    [teacher_deserialized_variables, student_deserialized_variables]\n",
    "                ]\n",
    "\n",
    "        self.student_config = student.config\n",
    "        self.vocab_size = student.config.vocab_size\n",
    "\n",
    "        # overwrite slightly on this.\n",
    "        if params.local_rank == -1:\n",
    "            sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "            \n",
    "        if params.group_by_size:\n",
    "            groups = create_lengths_groups(lengths=dataset.lengths, k=params.max_model_input_size)\n",
    "            sampler = GroupedBatchSampler(sampler=sampler, group_ids=groups, batch_size=params.batch_size)\n",
    "        else:\n",
    "            sampler = BatchSampler(sampler=sampler, batch_size=params.batch_size, drop_last=False)\n",
    "\n",
    "        self.dataloader = DataLoader(dataset=dataset, batch_sampler=sampler, collate_fn=dataset.batch_sequences)\n",
    "\n",
    "        self.temperature = params.temperature\n",
    "        assert self.temperature > 0.0\n",
    "\n",
    "        self.alpha_ce = params.alpha_ce\n",
    "        self.alpha_mlm = params.alpha_mlm\n",
    "        self.alpha_clm = params.alpha_clm\n",
    "        self.alpha_mse = params.alpha_mse\n",
    "        self.alpha_cos = params.alpha_cos\n",
    "        self.alpha_causal = params.alpha_causal\n",
    "\n",
    "        self.mlm = params.mlm\n",
    "        if self.mlm:\n",
    "            logger.info(\"Using MLM loss for LM step.\")\n",
    "            self.mlm_mask_prop = params.mlm_mask_prop\n",
    "            assert 0.0 <= self.mlm_mask_prop <= 1.0\n",
    "            assert params.word_mask + params.word_keep + params.word_rand == 1.0\n",
    "            self.pred_probs = torch.FloatTensor([params.word_mask, params.word_keep, params.word_rand])\n",
    "            self.pred_probs = self.pred_probs.to(torch.device(\"cuda\")) if params.n_gpu > 0 else self.pred_probs\n",
    "            self.token_probs = token_probs.to(torch.device(\"cuda\")) if params.n_gpu > 0 else token_probs\n",
    "            if self.fp16:\n",
    "                self.pred_probs = self.pred_probs.half()\n",
    "                self.token_probs = self.token_probs.half()\n",
    "        else:\n",
    "            logger.info(\"Using CLM loss for LM step.\")\n",
    "\n",
    "        self.interchange_mlm = params.interchange_mlm\n",
    "        self.interchange_prop = params.interchange_prop\n",
    "            \n",
    "        self.epoch = 0\n",
    "        self.n_iter = 0\n",
    "        self.n_total_iter = 0\n",
    "        self.n_sequences_epoch = 0\n",
    "        self.total_loss_epoch = 0\n",
    "        self.last_loss = 0\n",
    "        self.last_loss_ce = 0\n",
    "        self.last_loss_mlm = 0\n",
    "        self.last_loss_clm = 0\n",
    "        if self.alpha_mse > 0.0:\n",
    "            self.last_loss_mse = 0\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.last_loss_cos = 0\n",
    "\n",
    "        self.last_loss_causal_ce = 0\n",
    "        self.last_teacher_interchange_efficacy = 0\n",
    "        self.last_student_interchange_efficacy = 0\n",
    "        self.last_log = 0\n",
    "\n",
    "        self.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.lm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        if self.alpha_mse > 0.0:\n",
    "            self.mse_loss_fct = nn.MSELoss(reduction=\"sum\")\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "        logger.info(\"--- Initializing model optimizer\")\n",
    "        assert params.gradient_accumulation_steps >= 1\n",
    "        self.num_steps_epoch = len(self.dataloader)\n",
    "        num_train_optimization_steps = (\n",
    "            int(self.num_steps_epoch / params.gradient_accumulation_steps * params.n_epoch) + 1\n",
    "        )\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in student.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "                ],\n",
    "                \"weight_decay\": params.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in student.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        logger.info(\n",
    "            \"------ Number of trainable parameters (student): %i\"\n",
    "            % sum([p.numel() for p in self.student.parameters() if p.requires_grad])\n",
    "        )\n",
    "        logger.info(\"------ Number of parameters (student): %i\" % sum([p.numel() for p in self.student.parameters()]))\n",
    "        self.optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, lr=params.learning_rate, eps=params.adam_epsilon, betas=(0.9, 0.98)\n",
    "        )\n",
    "\n",
    "        warmup_steps = math.ceil(num_train_optimization_steps * params.warmup_prop)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps\n",
    "        )\n",
    "\n",
    "        if self.fp16:\n",
    "            try:\n",
    "                from apex import amp\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "            logger.info(f\"Using fp16 training: {self.params.fp16_opt_level} level\")\n",
    "            self.student, self.optimizer = amp.initialize(\n",
    "                self.student, self.optimizer, opt_level=self.params.fp16_opt_level\n",
    "            )\n",
    "            self.teacher = self.teacher.half()\n",
    "\n",
    "        if self.multi_gpu:\n",
    "            if self.fp16:\n",
    "                from apex.parallel import DistributedDataParallel\n",
    "\n",
    "                logger.info(\"Using apex.parallel.DistributedDataParallel for distributed training.\")\n",
    "                self.student = DistributedDataParallel(self.student)\n",
    "            else:\n",
    "                if params.local_rank == -1:\n",
    "                    logger.info(\"Using nn.DataParallel for the teacher model.\")\n",
    "                    # teacher also use multi-GPU.\n",
    "                    self.teacher = torch.nn.DataParallel(self.teacher)\n",
    "                    self.teacher.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "\n",
    "                    logger.info(\"Using nn.DataParallel for the student model.\")\n",
    "                    self.student = torch.nn.DataParallel(self.student)\n",
    "                    self.student.to(torch.device(\"cuda\")) # no rank is needed!\n",
    "                else:\n",
    "                \n",
    "                    from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "                    logger.info(\"Using nn.parallel.DistributedDataParallel for distributed training.\")\n",
    "                    self.student = DistributedDataParallel(\n",
    "                        self.student,\n",
    "                        device_ids=[params.local_rank],\n",
    "                        output_device=params.local_rank,\n",
    "                        find_unused_parameters=True,\n",
    "                    )\n",
    "\n",
    "        self.is_master = params.is_master\n",
    "        \n",
    "        \n",
    "    def prepare_batch_mlm(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare the batch: from the token_ids and the lengths, compute the attention mask and the masked label for MLM.\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "            batch: `Tuple`\n",
    "                token_ids: `torch.tensor(bs, seq_length)` - The token ids for each of the sequence. It is padded.\n",
    "                lengths: `torch.tensor(bs)` - The lengths of each of the sequences in the batch.\n",
    "\n",
    "        Output:\n",
    "        -------\n",
    "            token_ids: `torch.tensor(bs, seq_length)` - The token ids after the modifications for MLM.\n",
    "            attn_mask: `torch.tensor(bs, seq_length)` - The attention mask for the self-attention.\n",
    "            mlm_labels: `torch.tensor(bs, seq_length)` - The masked language modeling labels. There is a -100 where there is nothing to predict.\n",
    "        \"\"\"\n",
    "        token_ids, lengths = batch\n",
    "        token_ids, lengths = self.round_batch(x=token_ids, lengths=lengths)\n",
    "        assert token_ids.size(0) == lengths.size(0)\n",
    "\n",
    "        attn_mask = torch.arange(token_ids.size(1), dtype=torch.long, device=lengths.device) < lengths[:, None]\n",
    "\n",
    "        bs, max_seq_len = token_ids.size()\n",
    "        mlm_labels = token_ids.new(token_ids.size()).copy_(token_ids)\n",
    "\n",
    "        x_prob = self.token_probs[token_ids.flatten()]\n",
    "        n_tgt = math.ceil(self.mlm_mask_prop * lengths.sum().item())\n",
    "        tgt_ids = torch.multinomial(x_prob / x_prob.sum(), n_tgt, replacement=False)\n",
    "        pred_mask = torch.zeros(\n",
    "            bs * max_seq_len, dtype=torch.bool, device=token_ids.device\n",
    "        )  # previously `dtype=torch.uint8`, cf pytorch 1.2.0 compatibility\n",
    "        pred_mask[tgt_ids] = 1\n",
    "        pred_mask = pred_mask.view(bs, max_seq_len)\n",
    "\n",
    "        pred_mask[token_ids == self.params.special_tok_ids[\"pad_token\"]] = 0\n",
    "\n",
    "        # mask a number of words == 0 [8] (faster with fp16)\n",
    "        if self.fp16:\n",
    "            n1 = pred_mask.sum().item()\n",
    "            if n1 > 8:\n",
    "                pred_mask = pred_mask.view(-1)\n",
    "                n2 = max(n1 % 8, 8 * (n1 // 8))\n",
    "                if n2 != n1:\n",
    "                    pred_mask[torch.nonzero(pred_mask).view(-1)[: n1 - n2]] = 0\n",
    "                pred_mask = pred_mask.view(bs, max_seq_len)\n",
    "                assert pred_mask.sum().item() % 8 == 0, pred_mask.sum().item()\n",
    "\n",
    "        _token_ids_real = token_ids[pred_mask]\n",
    "        _token_ids_rand = _token_ids_real.clone().random_(self.vocab_size)\n",
    "        _token_ids_mask = _token_ids_real.clone().fill_(self.params.special_tok_ids[\"mask_token\"])\n",
    "        probs = torch.multinomial(self.pred_probs, len(_token_ids_real), replacement=True).to(_token_ids_real.device)\n",
    "        _token_ids = (\n",
    "            _token_ids_mask * (probs == 0).long()\n",
    "            + _token_ids_real * (probs == 1).long()\n",
    "            + _token_ids_rand * (probs == 2).long()\n",
    "        )\n",
    "        token_ids = token_ids.masked_scatter(pred_mask, _token_ids)\n",
    "\n",
    "        mlm_labels[~pred_mask] = -100  # previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility\n",
    "\n",
    "        # sanity checks\n",
    "        assert 0 <= token_ids.min() <= token_ids.max() < self.vocab_size\n",
    "\n",
    "        return token_ids, attn_mask, mlm_labels\n",
    "\n",
    "    def prepare_batch_clm(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare the batch: from the token_ids and the lengths, compute the attention mask and the labels for CLM.\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "            batch: `Tuple`\n",
    "                token_ids: `torch.tensor(bs, seq_length)` - The token ids for each of the sequence. It is padded.\n",
    "                lengths: `torch.tensor(bs)` - The lengths of each of the sequences in the batch.\n",
    "\n",
    "        Output:\n",
    "        -------\n",
    "            token_ids: `torch.tensor(bs, seq_length)` - The token ids after the modifications for MLM.\n",
    "            attn_mask: `torch.tensor(bs, seq_length)` - The attention mask for the self-attention.\n",
    "            clm_labels: `torch.tensor(bs, seq_length)` - The causal language modeling labels. There is a -100 where there is nothing to predict.\n",
    "        \"\"\"\n",
    "        token_ids, lengths = batch\n",
    "        token_ids, lengths = self.round_batch(x=token_ids, lengths=lengths)\n",
    "        assert token_ids.size(0) == lengths.size(0)\n",
    "\n",
    "        attn_mask = torch.arange(token_ids.size(1), dtype=torch.long, device=lengths.device) < lengths[:, None]\n",
    "        clm_labels = token_ids.new(token_ids.size()).copy_(token_ids)\n",
    "        clm_labels[~attn_mask] = -100  # previously `clm_labels[1-attn_mask] = -1`, cf pytorch 1.2.0 compatibility\n",
    "\n",
    "        # sanity checks\n",
    "        assert 0 <= token_ids.min() <= token_ids.max() < self.vocab_size\n",
    "\n",
    "        return token_ids, attn_mask, clm_labels\n",
    "\n",
    "    def round_batch(self, x: torch.tensor, lengths: torch.tensor):\n",
    "        \"\"\"\n",
    "        For float16 only.\n",
    "        Sub-sample sentences in a batch, and add padding, so that each dimension is a multiple of 8.\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "            x: `torch.tensor(bs, seq_length)` - The token ids.\n",
    "            lengths: `torch.tensor(bs, seq_length)` - The lengths of each of the sequence in the batch.\n",
    "\n",
    "        Output:\n",
    "        -------\n",
    "            x:  `torch.tensor(new_bs, new_seq_length)` - The updated token ids.\n",
    "            lengths: `torch.tensor(new_bs, new_seq_length)` - The updated lengths.\n",
    "        \"\"\"\n",
    "        if not self.fp16 or len(lengths) < 8:\n",
    "            return x, lengths\n",
    "\n",
    "        # number of sentences == 0 [8]\n",
    "        bs1 = len(lengths)\n",
    "        bs2 = 8 * (bs1 // 8)\n",
    "        assert bs2 > 0 and bs2 % 8 == 0\n",
    "        if bs1 != bs2:\n",
    "            idx = torch.randperm(bs1)[:bs2]\n",
    "            lengths = lengths[idx]\n",
    "            slen = lengths.max().item()\n",
    "            x = x[idx, :slen]\n",
    "        else:\n",
    "            idx = None\n",
    "\n",
    "        # sequence length == 0 [8]\n",
    "        ml1 = x.size(1)\n",
    "        if ml1 % 8 != 0:\n",
    "            pad = 8 - (ml1 % 8)\n",
    "            ml2 = ml1 + pad\n",
    "            if self.mlm:\n",
    "                pad_id = self.params.special_tok_ids[\"pad_token\"]\n",
    "            else:\n",
    "                pad_id = self.params.special_tok_ids[\"unk_token\"]\n",
    "            padding_tensor = torch.zeros(bs2, pad, dtype=torch.long, device=x.device).fill_(pad_id)\n",
    "            x = torch.cat([x, padding_tensor], 1)\n",
    "            assert x.size() == (bs2, ml2)\n",
    "\n",
    "        assert x.size(0) % 8 == 0\n",
    "        assert x.size(1) % 8 == 0\n",
    "        return x, lengths\n",
    "\n",
    "    def prepare_interchange_position(self, lengths, dual_lengths):\n",
    "        interchange_prop = self.interchange_prop\n",
    "        batch_size = lengths.shape[0]\n",
    "        interchange_position = []\n",
    "        for i in range(0, batch_size):\n",
    "            min_len = min(lengths[i].tolist(), dual_lengths[i].tolist())\n",
    "            interchange_count = int(min_len*interchange_prop)\n",
    "            start_index = random.randint(0, lengths[i].tolist()-interchange_count)\n",
    "            end_index = start_index + interchange_count\n",
    "            dual_start_index = random.randint(0, dual_lengths[i].tolist()-interchange_count)\n",
    "            dual_end_index = dual_start_index + interchange_count\n",
    "            interchange_position += [[start_index, end_index, dual_start_index, dual_end_index]]\n",
    "        interchange_position = torch.tensor(interchange_position, dtype=torch.long).to(lengths.device)\n",
    "        return interchange_position\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        The real training loop.\n",
    "        \"\"\"\n",
    "        if self.is_master:\n",
    "            logger.info(\"Starting training\")\n",
    "        self.last_log = time.time()\n",
    "        self.student.train()\n",
    "        self.teacher.eval()\n",
    "\n",
    "        for _ in range(self.params.n_epoch):\n",
    "            if self.is_master:\n",
    "                logger.info(f\"--- Starting epoch {self.epoch}/{self.params.n_epoch-1}\")\n",
    "\n",
    "            iter_bar = tqdm(self.dataloader, desc=\"-Iter\", disable=self.params.local_rank not in [-1, 0])\n",
    "            for batch in iter_bar:\n",
    "                token_ids, lengths, dual_token_ids, dual_lengths = batch\n",
    "\n",
    "                if self.params.n_gpu > 0:\n",
    "                    token_ids = token_ids.to(torch.device(\"cuda\"))\n",
    "                    lengths = lengths.to(torch.device(\"cuda\"))\n",
    "                    dual_token_ids = dual_token_ids.to(torch.device(\"cuda\"))\n",
    "                    dual_lengths = dual_lengths.to(torch.device(\"cuda\"))\n",
    "                \n",
    "                if self.mlm:\n",
    "                    token_ids, attn_mask, lm_labels = self.prepare_batch_mlm(batch=(token_ids, lengths))\n",
    "                    dual_token_ids, dual_attn_mask, dual_lm_labels = self.prepare_batch_mlm(\n",
    "                        batch=(dual_token_ids, dual_lengths)\n",
    "                    )\n",
    "                else:\n",
    "                    token_ids, attn_mask, lm_labels = self.prepare_batch_clm(batch=(token_ids, lengths))\n",
    "                    dual_token_ids, dual_attn_mask, dual_lm_labels = self.prepare_batch_clm(\n",
    "                        batch=(dual_token_ids, dual_lengths)\n",
    "                    )\n",
    "                    \n",
    "                # from length, let us get the intervention points?\n",
    "                sampled_interchange_position = self.prepare_interchange_position(lengths, dual_lengths)\n",
    "                \n",
    "                self.step(\n",
    "                    input_ids=token_ids, \n",
    "                    attention_mask=attn_mask, \n",
    "                    lm_labels=lm_labels,\n",
    "                    dual_input_ids=dual_token_ids, \n",
    "                    dual_attention_mask=dual_attn_mask, \n",
    "                    dual_lm_labels=dual_lm_labels,\n",
    "                    sampled_interchange_position=sampled_interchange_position,\n",
    "                    is_parallel=self.params.parallel_crossway,\n",
    "                    is_crossway=self.params.include_crossway,\n",
    "                )\n",
    "                iter_bar.update()\n",
    "                iter_bar.set_postfix(\n",
    "                    {\n",
    "                        \"Last_loss\": f\"{self.last_loss:.2f}\", \n",
    "                         \"Avg_cum_loss\": f\"{self.total_loss_epoch/self.n_iter:.2f}\", \n",
    "                         \"Last_cf_loss\": f\"{self.last_loss_causal_ce:.2f}\", \n",
    "                    }\n",
    "                )\n",
    "            iter_bar.close()\n",
    "\n",
    "            if self.is_master:\n",
    "                logger.info(f\"--- Ending epoch {self.epoch}/{self.params.n_epoch-1}\")\n",
    "            self.end_epoch()\n",
    "\n",
    "        if self.is_master:\n",
    "            logger.info(\"Save very last checkpoint as `pytorch_model.bin`.\")\n",
    "            self.save_checkpoint(checkpoint_name=\"pytorch_model.bin\")\n",
    "            logger.info(\"Training is finished\")\n",
    "\n",
    "    def step(\n",
    "        self, input_ids: torch.tensor, \n",
    "        attention_mask: torch.tensor, \n",
    "        lm_labels: torch.tensor,\n",
    "        dual_input_ids: torch.tensor, \n",
    "        dual_attention_mask: torch.tensor, \n",
    "        dual_lm_labels: torch.tensor,\n",
    "        sampled_interchange_position: torch.tensor,\n",
    "        is_parallel=False,\n",
    "        is_crossway=False,\n",
    "    ):\n",
    "        if is_parallel:\n",
    "            assert is_crossway\n",
    "            \"\"\"\n",
    "            If we enable crossway and parallel, we make \n",
    "            sure we compute pair-wise losses for two \n",
    "            examples together.\n",
    "            Note that this requires larger GPUs.\n",
    "            \"\"\"\n",
    "            self._step_parallel(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                lm_labels,\n",
    "                dual_input_ids,\n",
    "                dual_attention_mask,\n",
    "                dual_lm_labels,\n",
    "                sampled_interchange_position,\n",
    "            )\n",
    "        else:\n",
    "            \"\"\"\n",
    "            If it is not parallel, we will have two mini-step\n",
    "            within each step. The second step will only backprop\n",
    "            loss without updating the iteration, so the optimization\n",
    "            is not affected.\n",
    "            \"\"\"\n",
    "            if is_crossway:\n",
    "                self._step(\n",
    "                    input_ids,\n",
    "                    attention_mask,\n",
    "                    lm_labels,\n",
    "                    dual_input_ids,\n",
    "                    dual_attention_mask,\n",
    "                    dual_lm_labels,\n",
    "                    sampled_interchange_position,\n",
    "                    skip_update_iter=True,\n",
    "                )\n",
    "                # the second mini-step for the reversed pair.\n",
    "                self._step(\n",
    "                    dual_input_ids,\n",
    "                    dual_attention_mask,\n",
    "                    dual_lm_labels,\n",
    "                    input_ids,\n",
    "                    attention_mask,\n",
    "                    lm_labels,\n",
    "                    sampled_interchange_position,\n",
    "                    skip_update_iter=False,\n",
    "                )\n",
    "            else:\n",
    "                \"\"\"\n",
    "                This subroutine will be the normal distillation\n",
    "                with optional causal loss.\n",
    "                \"\"\"\n",
    "                self._step(\n",
    "                    input_ids,\n",
    "                    attention_mask,\n",
    "                    lm_labels,\n",
    "                    dual_input_ids,\n",
    "                    dual_attention_mask,\n",
    "                    dual_lm_labels,\n",
    "                    sampled_interchange_position,\n",
    "                    skip_update_iter=False,\n",
    "                )\n",
    "\n",
    "    def _step(\n",
    "        self, input_ids: torch.tensor, \n",
    "        attention_mask: torch.tensor, \n",
    "        lm_labels: torch.tensor,\n",
    "        dual_input_ids: torch.tensor, \n",
    "        dual_attention_mask: torch.tensor, \n",
    "        dual_lm_labels: torch.tensor,\n",
    "        sampled_interchange_position: torch.tensor,\n",
    "        skip_update_iter=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        One optimization step: forward of student AND teacher, backward on the loss (for gradient accumulation),\n",
    "        and possibly a parameter update (depending on the gradient accumulation).\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "        input_ids/dual_input_ids: `torch.tensor(bs, seq_length)` - The token ids.\n",
    "        attention_mask/dual_attention_mask: `torch.tensor(bs, seq_length)` - The attention mask for self attention.\n",
    "        lm_labels/dual_lm_labels: `torch.tensor(bs, seq_length)` - The language modeling labels (mlm labels for MLM and clm labels for CLM).\n",
    "        \"\"\"\n",
    "        # preparing for causal distillation.\n",
    "        # we randomly select the pool of neurons to interchange.\n",
    "        selector = random.randint(0, len(self.deserialized_interchange_variable_mappings)-1)\n",
    "        interchange_variable_mapping = self.deserialized_interchange_variable_mappings[selector]\n",
    "        teacher_variable_names = random.choice(interchange_variable_mapping[0])\n",
    "        student_variable_names = random.choice(interchange_variable_mapping[1])\n",
    "        teacher_interchanged_variables_mapping = {}\n",
    "        student_interchanged_variables_mapping = {}\n",
    "        # we need to do the interchange here.\n",
    "        for i, variable in enumerate(teacher_variable_names):\n",
    "            layer_index, head_index, LOC = parse_variable_name(variable)\n",
    "            if layer_index in teacher_interchanged_variables_mapping:\n",
    "                teacher_interchanged_variables_mapping[layer_index] += [(i, head_index, LOC)]\n",
    "            else:\n",
    "                teacher_interchanged_variables_mapping[layer_index] = [(i, head_index, LOC)]\n",
    "        for i, variable in enumerate(student_variable_names):\n",
    "            layer_index, head_index, LOC = parse_variable_name(variable)\n",
    "            if layer_index in student_interchanged_variables_mapping:\n",
    "                student_interchanged_variables_mapping[layer_index] += [(i, head_index, LOC)]\n",
    "            else:\n",
    "                student_interchanged_variables_mapping[layer_index] = [(i, head_index, LOC)]\n",
    "        \n",
    "        if self.mlm:\n",
    "            with torch.no_grad():\n",
    "                # teacher forward pass normal.\n",
    "                teacher_outputs = self.teacher(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask\n",
    "                )  # (bs, seq_length, voc_size)\n",
    "                # dual on main example\n",
    "                # teacher forward pass for interchange variables.\n",
    "                dual_counterfactual_activations_teacher = get_activation_at(\n",
    "                    self.teacher,\n",
    "                    dual_input_ids, # this is different!\n",
    "                    dual_attention_mask, # this is different!\n",
    "                    variable_names=teacher_variable_names\n",
    "                )\n",
    "                # teacher forward pass for interchanged outputs.\n",
    "                counterfactual_outputs_teacher = self.teacher(\n",
    "                    input_ids=input_ids, # this is different!\n",
    "                    attention_mask=attention_mask, # this is different!\n",
    "                    interchanged_variables=dual_counterfactual_activations_teacher,\n",
    "                    variable_names=teacher_interchanged_variables_mapping,\n",
    "                    sampled_interchange_position=sampled_interchange_position,\n",
    "                )\n",
    "            t_logits, t_hidden_states = \\\n",
    "                teacher_outputs[\"logits\"], teacher_outputs[\"hidden_states\"]\n",
    "            student_outputs = self.student(\n",
    "                input_ids=input_ids, attention_mask=attention_mask,\n",
    "                t_logits=t_logits,\n",
    "                t_hidden_states=t_hidden_states,\n",
    "                temperature=self.temperature,\n",
    "                restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "                lm_labels=lm_labels,\n",
    "                alpha_mlm=self.alpha_mlm,\n",
    "                alpha_clm=self.alpha_clm,\n",
    "                alpha_mse=self.alpha_mse,\n",
    "                alpha_cos=self.alpha_cos,\n",
    "            )  # (bs, seq_length, voc_size)\n",
    "            s_logits, s_hidden_states = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n",
    "            causal_t_logits, causal_t_hidden_states = \\\n",
    "                counterfactual_outputs_teacher[\"logits\"], counterfactual_outputs_teacher[\"hidden_states\"]\n",
    "        else:\n",
    "            assert False # we are not supporting this branch!\n",
    "        \n",
    "        # standard losses.\n",
    "        loss_ce = student_outputs[\"loss_ce\"].mean() if self.multi_gpu else student_outputs[\"loss_ce\"]\n",
    "        loss = self.alpha_ce * loss_ce\n",
    "\n",
    "        if self.alpha_mlm > 0.0:\n",
    "            loss_mlm = student_outputs[\"loss_mlm\"].mean() if self.multi_gpu else student_outputs[\"loss_mlm\"]\n",
    "            loss += self.alpha_mlm * loss_mlm\n",
    "        if self.alpha_clm > 0.0:\n",
    "            loss_clm = student_outputs[\"loss_clm\"].mean() if self.multi_gpu else student_outputs[\"loss_clm\"]\n",
    "            loss += self.alpha_clm * loss_clm\n",
    "        if self.alpha_mse > 0.0:\n",
    "            loss_mse = student_outputs[\"loss_mse\"].mean() if self.multi_gpu else student_outputs[\"loss_mse\"]\n",
    "            loss += self.alpha_mse * loss_mse\n",
    "        if self.alpha_cos > 0.0:\n",
    "            loss_cos = student_outputs[\"loss_cos\"].mean() if self.multi_gpu else student_outputs[\"loss_cos\"]\n",
    "            loss += self.alpha_cos * loss_cos\n",
    "            \n",
    "        # we need to get causal distillation loss!\n",
    "        dual_counterfactual_activations_student = get_activation_at(\n",
    "            self.student,\n",
    "            dual_input_ids, # this is different!\n",
    "            dual_attention_mask, # this is different!\n",
    "            variable_names=student_variable_names\n",
    "        )\n",
    "        # dual on main.\n",
    "        counterfactual_outputs_student = self.student(\n",
    "            input_ids=input_ids, # this is different!\n",
    "            attention_mask=attention_mask, # this is different!\n",
    "            # interchange.\n",
    "            interchanged_variables=dual_counterfactual_activations_student,\n",
    "            variable_names=student_interchanged_variables_mapping,\n",
    "            sampled_interchange_position=sampled_interchange_position,\n",
    "            # loss.\n",
    "            t_logits=t_logits,\n",
    "            t_hidden_states=t_hidden_states,\n",
    "            causal_t_logits=causal_t_logits,\n",
    "            causal_t_hidden_states=causal_t_hidden_states,\n",
    "            s_logits=s_logits,\n",
    "            s_hidden_states=s_hidden_states,\n",
    "            temperature=self.temperature,\n",
    "            restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "        )\n",
    "        # sanity check.\n",
    "        assert \"loss_ce\" not in counterfactual_outputs_student\n",
    "        assert \"loss_mlm\" not in counterfactual_outputs_student\n",
    "        assert \"loss_clm\" not in counterfactual_outputs_student\n",
    "        assert \"loss_mse\" not in counterfactual_outputs_student\n",
    "        assert \"loss_cos\" not in counterfactual_outputs_student\n",
    "        causal_loss_ce = counterfactual_outputs_student[\"causal_loss_ce\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"causal_loss_ce\"]\n",
    "        teacher_interchange_efficacy = \\\n",
    "            counterfactual_outputs_student[\"teacher_interchange_efficacy\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"teacher_interchange_efficacy\"]\n",
    "        student_interchange_efficacy = \\\n",
    "            counterfactual_outputs_student[\"student_interchange_efficacy\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"student_interchange_efficacy\"]\n",
    "        if self.alpha_causal > 0.0:\n",
    "            loss += self.alpha_causal * causal_loss_ce\n",
    "                \n",
    "        self.total_loss_epoch += loss.item()\n",
    "        self.last_loss = loss.item()\n",
    "        self.last_loss_ce = loss_ce.item()\n",
    "        if self.alpha_mlm > 0.0:\n",
    "            self.last_loss_mlm = loss_mlm.item()\n",
    "        if self.alpha_clm > 0.0:\n",
    "            self.last_loss_clm = loss_clm.item()\n",
    "        if self.alpha_mse > 0.0:\n",
    "            self.last_loss_mse = loss_mse.item()\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.last_loss_cos = loss_cos.item()\n",
    "        # optional recording of the value.\n",
    "        self.last_loss_causal_ce = causal_loss_ce.item()\n",
    "        # record efficacy of the interchange.\n",
    "        self.last_teacher_interchange_efficacy = teacher_interchange_efficacy.item()\n",
    "        self.last_student_interchange_efficacy = student_interchange_efficacy.item()\n",
    "            \n",
    "        self.optimize(loss, skip_update_iter=skip_update_iter)\n",
    "\n",
    "        self.n_sequences_epoch += input_ids.size(0)\n",
    "        \n",
    "    def _step_parallel(\n",
    "        self, input_ids: torch.tensor, \n",
    "        attention_mask: torch.tensor, \n",
    "        lm_labels: torch.tensor,\n",
    "        dual_input_ids: torch.tensor, \n",
    "        dual_attention_mask: torch.tensor, \n",
    "        dual_lm_labels: torch.tensor,\n",
    "        sampled_interchange_position: torch.tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        WARNING: Parallel requires GPUs with larger memory. It involves computations across two examples\n",
    "        with two parallel iterations.\n",
    "        \n",
    "        One optimization step: forward of student AND teacher, backward on the loss (for gradient accumulation),\n",
    "        and possibly a parameter update (depending on the gradient accumulation).\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "        input_ids/dual_input_ids: `torch.tensor(bs, seq_length)` - The token ids.\n",
    "        attention_mask/dual_attention_mask: `torch.tensor(bs, seq_length)` - The attention mask for self attention.\n",
    "        lm_labels/dual_lm_labels: `torch.tensor(bs, seq_length)` - The language modeling labels (mlm labels for MLM and clm labels for CLM).\n",
    "        \"\"\"\n",
    "        \n",
    "        # preparing for causal distillation.\n",
    "        # we randomly select the pool of neurons to interchange.\n",
    "        selector = random.randint(0, len(self.deserialized_interchange_variable_mappings)-1)\n",
    "        interchange_variable_mapping = self.deserialized_interchange_variable_mappings[selector]\n",
    "        teacher_variable_names = random.choice(interchange_variable_mapping[0])\n",
    "        student_variable_names = random.choice(interchange_variable_mapping[1])\n",
    "        teacher_interchanged_variables_mapping = {}\n",
    "        student_interchanged_variables_mapping = {}\n",
    "        # we need to do the interchange here.\n",
    "        for i, variable in enumerate(teacher_variable_names):\n",
    "            layer_index, head_index, LOC = parse_variable_name(variable)\n",
    "            if layer_index in teacher_interchanged_variables_mapping:\n",
    "                teacher_interchanged_variables_mapping[layer_index] += [(i, head_index, LOC)]\n",
    "            else:\n",
    "                teacher_interchanged_variables_mapping[layer_index] = [(i, head_index, LOC)]\n",
    "        for i, variable in enumerate(student_variable_names):\n",
    "            layer_index, head_index, LOC = parse_variable_name(variable)\n",
    "            if layer_index in student_interchanged_variables_mapping:\n",
    "                student_interchanged_variables_mapping[layer_index] += [(i, head_index, LOC)]\n",
    "            else:\n",
    "                student_interchanged_variables_mapping[layer_index] = [(i, head_index, LOC)]\n",
    "        \n",
    "        if self.mlm:\n",
    "            with torch.no_grad():\n",
    "                # teacher forward pass normal.\n",
    "                teacher_outputs = self.teacher(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask\n",
    "                )  # (bs, seq_length, voc_size)\n",
    "                # teacher forward pass normal for the dual example.\n",
    "                dual_teacher_outputs = self.teacher(\n",
    "                    input_ids=dual_input_ids, attention_mask=dual_attention_mask\n",
    "                )  # (bs, seq_length, voc_size)\n",
    "                \n",
    "                # dual on main example\n",
    "                # teacher forward pass for interchange variables.\n",
    "                dual_counterfactual_activations_teacher = get_activation_at(\n",
    "                    self.teacher,\n",
    "                    dual_input_ids, # this is different!\n",
    "                    dual_attention_mask, # this is different!\n",
    "                    variable_names=teacher_variable_names\n",
    "                )\n",
    "                # teacher forward pass for interchanged outputs.\n",
    "                counterfactual_outputs_teacher = self.teacher(\n",
    "                    input_ids=input_ids, # this is different!\n",
    "                    attention_mask=attention_mask, # this is different!\n",
    "                    interchanged_variables=dual_counterfactual_activations_teacher,\n",
    "                    variable_names=teacher_interchanged_variables_mapping,\n",
    "                    sampled_interchange_position=sampled_interchange_position,\n",
    "                )   \n",
    "                \n",
    "                # main on dual example\n",
    "                # teacher forward pass for interchange variables.\n",
    "                counterfactual_activations_teacher = get_activation_at(\n",
    "                    self.teacher,\n",
    "                    input_ids, # this is different!\n",
    "                    attention_mask, # this is different!\n",
    "                    variable_names=teacher_variable_names\n",
    "                )\n",
    "                # teacher forward pass for interchanged outputs.\n",
    "                dual_counterfactual_outputs_teacher = self.teacher(\n",
    "                    input_ids=dual_input_ids, # this is different!\n",
    "                    attention_mask=dual_attention_mask, # this is different!\n",
    "                    interchanged_variables=counterfactual_activations_teacher,\n",
    "                    variable_names=teacher_interchanged_variables_mapping,\n",
    "                    sampled_interchange_position=sampled_interchange_position,\n",
    "                )\n",
    "            t_logits, t_hidden_states = \\\n",
    "                teacher_outputs[\"logits\"], teacher_outputs[\"hidden_states\"]\n",
    "            dual_t_logits, dual_t_hidden_states = \\\n",
    "                dual_teacher_outputs[\"logits\"], dual_teacher_outputs[\"hidden_states\"]\n",
    "            student_outputs = self.student(\n",
    "                input_ids=input_ids, attention_mask=attention_mask,\n",
    "                t_logits=t_logits,\n",
    "                t_hidden_states=t_hidden_states,\n",
    "                temperature=self.temperature,\n",
    "                restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "                lm_labels=lm_labels,\n",
    "                alpha_mlm=self.alpha_mlm,\n",
    "                alpha_clm=self.alpha_clm,\n",
    "                alpha_mse=self.alpha_mse,\n",
    "                alpha_cos=self.alpha_cos,\n",
    "            )  # (bs, seq_length, voc_size)\n",
    "            dual_student_outputs = self.student(\n",
    "                input_ids=dual_input_ids, attention_mask=dual_attention_mask,\n",
    "                t_logits=dual_t_logits,\n",
    "                t_hidden_states=dual_t_hidden_states,\n",
    "                temperature=self.temperature,\n",
    "                restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "                lm_labels=lm_labels,\n",
    "                alpha_mlm=self.alpha_mlm,\n",
    "                alpha_clm=self.alpha_clm,\n",
    "                alpha_mse=self.alpha_mse,\n",
    "                alpha_cos=self.alpha_cos,\n",
    "            )  # (bs, seq_length, voc_size)\n",
    "            s_logits, s_hidden_states = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n",
    "            dual_s_logits, dual_s_hidden_states = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n",
    "            causal_t_logits, causal_t_hidden_states = \\\n",
    "                counterfactual_outputs_teacher[\"logits\"], counterfactual_outputs_teacher[\"hidden_states\"]\n",
    "            dual_causal_t_logits, dual_causal_t_hidden_states = \\\n",
    "                counterfactual_outputs_teacher[\"logits\"], counterfactual_outputs_teacher[\"hidden_states\"]\n",
    "        else:\n",
    "            assert False # we are not supporting this branch!\n",
    "        \n",
    "        # standard losses.\n",
    "        loss_ce = student_outputs[\"loss_ce\"].mean() if self.multi_gpu else student_outputs[\"loss_ce\"]\n",
    "        loss_ce += dual_student_outputs[\"loss_ce\"].mean() if self.multi_gpu else dual_student_outputs[\"loss_ce\"]\n",
    "        loss = self.alpha_ce * loss_ce\n",
    "\n",
    "        if self.alpha_mlm > 0.0:\n",
    "            loss_mlm = student_outputs[\"loss_mlm\"].mean() if self.multi_gpu else student_outputs[\"loss_mlm\"]\n",
    "            loss_mlm += dual_student_outputs[\"loss_mlm\"].mean() if self.multi_gpu else dual_student_outputs[\"loss_mlm\"]\n",
    "            loss += self.alpha_mlm * loss_mlm\n",
    "        if self.alpha_clm > 0.0:\n",
    "            loss_clm = student_outputs[\"loss_clm\"].mean() if self.multi_gpu else student_outputs[\"loss_clm\"]\n",
    "            loss_clm += dual_student_outputs[\"loss_clm\"].mean() if self.multi_gpu else dual_student_outputs[\"loss_clm\"]\n",
    "            loss += self.alpha_clm * loss_clm\n",
    "        if self.alpha_mse > 0.0:\n",
    "            loss_mse = student_outputs[\"loss_mse\"].mean() if self.multi_gpu else student_outputs[\"loss_mse\"]\n",
    "            loss_mse += dual_student_outputs[\"loss_mse\"].mean() if self.multi_gpu else dual_student_outputs[\"loss_mse\"]\n",
    "            loss += self.alpha_mse * loss_mse\n",
    "        if self.alpha_cos > 0.0:\n",
    "            loss_cos = student_outputs[\"loss_cos\"].mean() if self.multi_gpu else student_outputs[\"loss_cos\"]\n",
    "            loss_cos += dual_student_outputs[\"loss_cos\"].mean() if self.multi_gpu else dual_student_outputs[\"loss_cos\"]\n",
    "            loss += self.alpha_cos * loss_cos\n",
    "            \n",
    "       # we need to get causal distillation loss!\n",
    "        dual_counterfactual_activations_student = get_activation_at(\n",
    "            self.student,\n",
    "            dual_input_ids, # this is different!\n",
    "            dual_attention_mask, # this is different!\n",
    "            variable_names=student_variable_names\n",
    "        )\n",
    "        counterfactual_activations_student = get_activation_at(\n",
    "            self.student,\n",
    "            input_ids, # this is different!\n",
    "            attention_mask, # this is different!\n",
    "            variable_names=student_variable_names\n",
    "        )\n",
    "        # dual on main.\n",
    "        counterfactual_outputs_student = self.student(\n",
    "            input_ids=input_ids, # this is different!\n",
    "            attention_mask=attention_mask, # this is different!\n",
    "            # interchange.\n",
    "            interchanged_variables=dual_counterfactual_activations_student,\n",
    "            variable_names=student_interchanged_variables_mapping,\n",
    "            sampled_interchange_position=sampled_interchange_position,\n",
    "            # loss.\n",
    "            t_logits=t_logits,\n",
    "            t_hidden_states=t_hidden_states,\n",
    "            causal_t_logits=causal_t_logits,\n",
    "            causal_t_hidden_states=causal_t_hidden_states,\n",
    "            s_logits=s_logits,\n",
    "            s_hidden_states=s_hidden_states,\n",
    "            temperature=self.temperature,\n",
    "            restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "        )\n",
    "        # main on dual.\n",
    "        dual_counterfactual_outputs_student = self.student(\n",
    "            input_ids=dual_input_ids, # this is different!\n",
    "            attention_mask=dual_attention_mask, # this is different!\n",
    "            # interchange.\n",
    "            interchanged_variables=dual_counterfactual_activations_student,\n",
    "            variable_names=student_interchanged_variables_mapping,\n",
    "            sampled_interchange_position=sampled_interchange_position,\n",
    "            # loss.\n",
    "            t_logits=dual_t_logits,\n",
    "            t_hidden_states=dual_t_hidden_states,\n",
    "            causal_t_logits=dual_causal_t_logits,\n",
    "            causal_t_hidden_states=dual_causal_t_hidden_states,\n",
    "            s_logits=dual_s_logits,\n",
    "            s_hidden_states=dual_s_hidden_states,\n",
    "            temperature=self.temperature,\n",
    "            restrict_ce_to_mask=self.params.restrict_ce_to_mask,\n",
    "        )\n",
    "        \n",
    "        # sanity check.\n",
    "        assert \"loss_ce\" not in counterfactual_outputs_student and \"loss_ce\" not in dual_counterfactual_outputs_student\n",
    "        assert \"loss_mlm\" not in counterfactual_outputs_student and \"loss_mlm\" not in dual_counterfactual_outputs_student\n",
    "        assert \"loss_clm\" not in counterfactual_outputs_student and \"loss_clm\" not in dual_counterfactual_outputs_student\n",
    "        assert \"loss_mse\" not in counterfactual_outputs_student and \"loss_mse\" not in dual_counterfactual_outputs_student\n",
    "        assert \"loss_cos\" not in counterfactual_outputs_student and \"loss_cos\" not in dual_counterfactual_outputs_student\n",
    "        causal_loss_ce = counterfactual_outputs_student[\"causal_loss_ce\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"causal_loss_ce\"]\n",
    "        causal_loss_ce += dual_counterfactual_outputs_student[\"causal_loss_ce\"].mean() if self.multi_gpu else dual_counterfactual_outputs_student[\"causal_loss_ce\"]\n",
    "        teacher_interchange_efficacy = \\\n",
    "            counterfactual_outputs_student[\"teacher_interchange_efficacy\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"teacher_interchange_efficacy\"]\n",
    "        student_interchange_efficacy = \\\n",
    "            counterfactual_outputs_student[\"student_interchange_efficacy\"].mean() if self.multi_gpu else counterfactual_outputs_student[\"student_interchange_efficacy\"]\n",
    "        teacher_interchange_efficacy += \\\n",
    "            dual_counterfactual_outputs_student[\"teacher_interchange_efficacy\"].mean() if self.multi_gpu else dual_counterfactual_outputs_student[\"teacher_interchange_efficacy\"]\n",
    "        student_interchange_efficacy += \\\n",
    "            dual_counterfactual_outputs_student[\"student_interchange_efficacy\"].mean() if self.multi_gpu else dual_counterfactual_outputs_student[\"student_interchange_efficacy\"]\n",
    "        if self.alpha_causal > 0.0:\n",
    "            loss += self.alpha_causal * causal_loss_ce\n",
    "                \n",
    "        self.total_loss_epoch += loss.item()\n",
    "        self.last_loss = loss.item()\n",
    "        self.last_loss_ce = loss_ce.item()\n",
    "        if self.alpha_mlm > 0.0:\n",
    "            self.last_loss_mlm = loss_mlm.item()\n",
    "        if self.alpha_clm > 0.0:\n",
    "            self.last_loss_clm = loss_clm.item()\n",
    "        if self.alpha_mse > 0.0:\n",
    "            self.last_loss_mse = loss_mse.item()\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.last_loss_cos = loss_cos.item()\n",
    "        # optional recording of the value.\n",
    "        self.last_loss_causal_ce = causal_loss_ce.item()\n",
    "        # record efficacy of the interchange.\n",
    "        self.last_teacher_interchange_efficacy = teacher_interchange_efficacy.item()\n",
    "        self.last_student_interchange_efficacy = student_interchange_efficacy.item()\n",
    "            \n",
    "        self.optimize(loss)\n",
    "\n",
    "        self.n_sequences_epoch += input_ids.size(0)\n",
    "\n",
    "    def optimize(self, loss, skip_update_iter=False):\n",
    "        \"\"\"\n",
    "        Normalization on the loss (gradient accumulation or distributed training), followed by\n",
    "        backward pass on the loss, possibly followed by a parameter update (depending on the gradient accumulation).\n",
    "        Also update the metrics for tensorboard.\n",
    "        \"\"\"\n",
    "        # Check for NaN\n",
    "        if (loss != loss).data.any():\n",
    "            logger.error(\"NaN detected\")\n",
    "            exit()\n",
    "\n",
    "        if self.multi_gpu:\n",
    "            loss = loss.mean()\n",
    "        if self.params.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.params.gradient_accumulation_steps\n",
    "\n",
    "        if self.fp16:\n",
    "            from apex import amp\n",
    "\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        \"\"\"\n",
    "        In case where we want to do two mini-steps for dual on main interchange,\n",
    "        and main on dual interchange (including normal objectives), we want to\n",
    "        skip the iter update, so the gradients are accumulated within the step\n",
    "        which includes gradients from two mini-steps.\n",
    "        \"\"\"\n",
    "        self.iter(skip_update_iter=skip_update_iter)\n",
    "\n",
    "        if self.n_iter % self.params.gradient_accumulation_steps == 0:\n",
    "            if self.fp16:\n",
    "                nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), self.params.max_grad_norm)\n",
    "            else:\n",
    "                nn.utils.clip_grad_norm_(self.student.parameters(), self.params.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def iter(self, skip_update_iter=False):\n",
    "        \"\"\"\n",
    "        Update global counts, write to tensorboard and save checkpoint.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not skip_update_iter:\n",
    "            self.n_iter += 1\n",
    "            self.n_total_iter += 1\n",
    "            if self.n_total_iter % self.params.checkpoint_interval == 0:\n",
    "                self.save_checkpoint()\n",
    "        \n",
    "        \"\"\"\n",
    "        Logging is not affected by the flag skip_update_iter.\n",
    "        We want to log crossway effects, and losses should be\n",
    "        in the same magnitude.\n",
    "        \"\"\"\n",
    "        if self.n_total_iter % self.params.log_interval == 0:\n",
    "            self.log_tensorboard()\n",
    "            self.last_log = time.time()\n",
    "\n",
    "    def log_tensorboard(self):\n",
    "        \"\"\"\n",
    "        Log into tensorboard. Only by the master process.\n",
    "        \"\"\"\n",
    "        if not self.is_master:\n",
    "            return\n",
    "        \n",
    "        if not self.is_wandb:\n",
    "            return\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/cum_avg_loss_epoch\": self.total_loss_epoch / self.n_iter, \n",
    "                \"train/loss\": self.last_loss, \n",
    "                \"train/loss_ce\": self.last_loss_ce, \n",
    "            }, \n",
    "            step=self.n_total_iter\n",
    "        )\n",
    "        \n",
    "        if self.alpha_mlm > 0.0:\n",
    "            wandb.log(\n",
    "                {\"train/loss_mlm\": self.last_loss_mlm}, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "        if self.alpha_clm > 0.0:\n",
    "            wandb.log(\n",
    "                {\"train/loss_clm\": self.last_loss_clm}, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "        if self.alpha_mse > 0.0:\n",
    "            wandb.log(\n",
    "                {\"train/loss_mse\": self.last_loss_mse}, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "        if self.alpha_cos > 0.0:\n",
    "            wandb.log(\n",
    "                {\"train/loss_cos\": self.last_loss_cos}, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss_causal_ce\": self.last_loss_causal_ce,\n",
    "                \"train/teacher_interchange_efficacy\": self.last_teacher_interchange_efficacy,\n",
    "                \"train/student_interchange_efficacy\": self.last_student_interchange_efficacy,\n",
    "            }, \n",
    "            step=self.n_total_iter\n",
    "        )\n",
    "        \n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/learning_rate\": self.scheduler.get_lr()[0],\n",
    "                \"train/memory_usage\": psutil.virtual_memory()._asdict()[\"used\"] / 1_000_000,\n",
    "                \"train/speed\": time.time() - self.last_log,\n",
    "            }, \n",
    "            step=self.n_total_iter\n",
    "        )\n",
    "\n",
    "    def end_epoch(self):\n",
    "        \"\"\"\n",
    "        Finally arrived at the end of epoch (full pass on dataset).\n",
    "        Do some tensorboard logging and checkpoint saving.\n",
    "        \"\"\"\n",
    "        logger.info(f\"{self.n_sequences_epoch} sequences have been trained during this epoch.\")\n",
    "\n",
    "        if self.is_master:\n",
    "            self.save_checkpoint(checkpoint_name=f\"model_epoch_{self.epoch}.pth\")\n",
    "            if self.is_wandb:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch/loss\": self.total_loss_epoch / self.n_iter, \n",
    "                        'epoch': self.epoch\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        self.epoch += 1\n",
    "        self.n_sequences_epoch = 0\n",
    "        self.n_iter = 0\n",
    "        self.total_loss_epoch = 0\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_name: str = \"checkpoint.pth\"):\n",
    "        \"\"\"\n",
    "        Save the current state. Only by the master process.\n",
    "        \"\"\"\n",
    "        if not self.is_master:\n",
    "            return\n",
    "        mdl_to_save = self.student.module if hasattr(self.student, \"module\") else self.student\n",
    "        mdl_to_save.config.save_pretrained(self.dump_path)\n",
    "        state_dict = mdl_to_save.state_dict()\n",
    "        torch.save(state_dict, os.path.join(self.dump_path, checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_checks(args):\n",
    "    \"\"\"\n",
    "    A bunch of args sanity checks to perform even starting...\n",
    "    \"\"\"\n",
    "    assert (args.mlm and args.alpha_mlm > 0.0) or (not args.mlm and args.alpha_mlm == 0.0)\n",
    "    assert (args.alpha_mlm > 0.0 and args.alpha_clm == 0.0) or (args.alpha_mlm == 0.0 and args.alpha_clm > 0.0)\n",
    "    if args.mlm:\n",
    "        assert os.path.isfile(args.token_counts)\n",
    "        assert (args.student_type in [\"roberta\", \"distilbert\"]) and (args.teacher_type in [\"roberta\", \"bert\"])\n",
    "    else:\n",
    "        assert (args.student_type in [\"gpt2\"]) and (args.teacher_type in [\"gpt2\"])\n",
    "\n",
    "    assert args.teacher_type == args.student_type or (\n",
    "        args.student_type == \"distilbert\" and args.teacher_type == \"bert\"\n",
    "    )\n",
    "    assert os.path.isfile(args.student_config)\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        assert os.path.isfile(args.student_pretrained_weights)\n",
    "\n",
    "    if args.freeze_token_type_embds:\n",
    "        assert args.student_type in [\"roberta\"]\n",
    "\n",
    "    assert args.alpha_ce >= 0.0\n",
    "    assert args.alpha_mlm >= 0.0\n",
    "    assert args.alpha_clm >= 0.0\n",
    "    assert args.alpha_mse >= 0.0\n",
    "    assert args.alpha_cos >= 0.0\n",
    "    assert args.alpha_causal >= 0.0\n",
    "    assert args.alpha_ce + args.alpha_mlm + args.alpha_clm + args.alpha_mse + args.alpha_cos + args.alpha_causal > 0.0\n",
    "\n",
    "\n",
    "def freeze_pos_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "    elif args.student_type == \"gpt2\":\n",
    "        student.transformer.wpe.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def freeze_token_type_embeddings(student, args):\n",
    "    if args.student_type == \"roberta\":\n",
    "        student.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "\n",
    "\n",
    "def prepare_distiller(args):\n",
    "\n",
    "    # ARGS #\n",
    "    init_gpu_params(args)\n",
    "    set_seed(args)\n",
    "    # More validations #\n",
    "    if args.parallel_crossway:\n",
    "        assert args.include_crossway\n",
    "    if not args.include_crossway:\n",
    "        assert not args.parallel_crossway\n",
    "    if args.is_master:\n",
    "        if os.path.exists(args.dump_path):\n",
    "            if not args.force:\n",
    "                raise ValueError(\n",
    "                    f\"Serialization dir {args.dump_path} already exists, but you have not precised wheter to overwrite it\"\n",
    "                    \"Use `--force` if you want to overwrite it\"\n",
    "                )\n",
    "            else:\n",
    "                shutil.rmtree(args.dump_path)\n",
    "\n",
    "        if not os.path.exists(args.dump_path):\n",
    "            os.makedirs(args.dump_path)\n",
    "        logger.info(f\"Experiment will be dumped and logged in {args.dump_path}\")\n",
    "\n",
    "        # SAVE PARAMS #\n",
    "        logger.info(f\"Param: {args}\")\n",
    "        with open(os.path.join(args.dump_path, \"parameters.json\"), \"w\") as f:\n",
    "            json.dump(vars(args), f, indent=4)\n",
    "        git_log(args.dump_path)\n",
    "\n",
    "    student_config_class, student_model_class, _ = MODEL_CLASSES[args.student_type]\n",
    "    teacher_config_class, teacher_model_class, teacher_tokenizer_class = MODEL_CLASSES[args.teacher_type]\n",
    "\n",
    "    # TOKENIZER #\n",
    "    tokenizer = teacher_tokenizer_class.from_pretrained(\n",
    "        args.teacher_name,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    special_tok_ids = {}\n",
    "    for tok_name, tok_symbol in tokenizer.special_tokens_map.items():\n",
    "        idx = tokenizer.all_special_tokens.index(tok_symbol)\n",
    "        special_tok_ids[tok_name] = tokenizer.all_special_ids[idx]\n",
    "    logger.info(f\"Special tokens {special_tok_ids}\")\n",
    "    args.special_tok_ids = special_tok_ids\n",
    "    args.max_model_input_size = tokenizer.max_model_input_sizes[args.teacher_name]\n",
    "\n",
    "    # DATA LOADER #\n",
    "    logger.info(f\"Loading data from {args.data_file}\")\n",
    "    with open(args.data_file, \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    if args.mlm:\n",
    "        logger.info(f\"Loading token counts from {args.token_counts} (already pre-computed)\")\n",
    "        with open(args.token_counts, \"rb\") as fp:\n",
    "            counts = pickle.load(fp)\n",
    "\n",
    "        token_probs = np.maximum(counts, 1) ** -args.mlm_smoothing\n",
    "        for idx in special_tok_ids.values():\n",
    "            token_probs[idx] = 0.0  # do not predict special tokens\n",
    "        token_probs = torch.from_numpy(token_probs)\n",
    "    else:\n",
    "        token_probs = None\n",
    "\n",
    "    train_lm_seq_dataset = LmSeqsDataset(params=args, data=data)\n",
    "    logger.info(\"Data loader created.\")\n",
    "\n",
    "    # STUDENT #\n",
    "    logger.info(f\"Loading student config from {args.student_config}\")\n",
    "    stu_architecture_config = student_config_class.from_pretrained(\n",
    "        args.student_config,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    stu_architecture_config.output_hidden_states = True\n",
    "\n",
    "    if args.student_pretrained_weights is not None:\n",
    "        logger.info(f\"Loading pretrained weights from {args.student_pretrained_weights}\")\n",
    "        student = student_model_class.from_pretrained(\n",
    "            args.student_pretrained_weights, config=stu_architecture_config,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "    else:\n",
    "        student = student_model_class(stu_architecture_config)\n",
    "    logger.info(\"Student loaded.\")\n",
    "\n",
    "    # TEACHER #\n",
    "    teacher = teacher_model_class.from_pretrained(\n",
    "        args.teacher_name, output_hidden_states=True, \n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Teacher loaded from {args.teacher_name}.\")\n",
    "\n",
    "    # FREEZING #\n",
    "    if args.freeze_pos_embs:\n",
    "        freeze_pos_embeddings(student, args)\n",
    "    if args.freeze_token_type_embds:\n",
    "        freeze_token_type_embeddings(student, args)\n",
    "\n",
    "    # SANITY CHECKS #\n",
    "    assert student.config.vocab_size == teacher.config.vocab_size\n",
    "    assert student.config.hidden_size == teacher.config.hidden_size\n",
    "    assert student.config.max_position_embeddings == teacher.config.max_position_embeddings\n",
    "    if args.mlm:\n",
    "        assert token_probs.size(0) == stu_architecture_config.vocab_size\n",
    "\n",
    "    # DISTILLER #\n",
    "    torch.cuda.empty_cache()\n",
    "    distiller = CausalDistiller(\n",
    "        params=args, dataset=train_lm_seq_dataset, token_probs=token_probs, student=student, teacher=teacher\n",
    "    )\n",
    "    logger.info(\"Distiller initialization done.\")\n",
    "    return distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2021 11:57:57 - INFO - utils - PID: 51231 -  Experiment will be dumped and logged in ./arxiv_results/s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25_nm_single_multilayer_crossway_False\n",
      "11/15/2021 11:57:57 - INFO - utils - PID: 51231 -  Param: Namespace(adam_epsilon=1e-06, alpha_causal=0.25, alpha_ce=0.25, alpha_clm=0.0, alpha_cos=0.25, alpha_mlm=0.25, alpha_mse=0.0, batch_size=5, cache_dir='./distill_cache/', checkpoint_interval=4000, data_file='./demo_data/binarized_text.train.bert-base-uncased.pickle', dump_path='./arxiv_results/s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25_nm_single_multilayer_crossway_False', force=True, fp16=False, fp16_opt_level='O1', freeze_pos_embs=False, freeze_token_type_embds=False, gradient_accumulation_steps=50, group_by_size=True, include_crossway=False, initializer_range=0.02, interchange_mlm=False, interchange_prop=0.3, is_master=True, is_wandb=False, learning_rate=0.0005, local_rank=-1, log_interval=10, master_port=-1, max_grad_norm=5.0, mlm=True, mlm_mask_prop=0.15, mlm_smoothing=0.7, multi_gpu=False, n_epoch=3, n_gpu=0, neuron_mapping='./training_configs/single_multilayer.nm', parallel_crossway=False, restrict_ce_to_mask=False, run_name='s_distilbert_t_bert_data_demo_data_seed_56_mlm_True_ce_0.25_mlm_0.25_cos_0.25_causal_0.25_nm_single_multilayer_crossway_False', seed=56, student_config='./training_configs/distilbert-base-uncased.json', student_pretrained_weights=None, student_type='distilbert', teacher_name='bert-base-uncased', teacher_type='bert', temperature=2.0, token_counts='./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle', warmup_prop=0.05, weight_decay=0.0, word_keep=0.1, word_mask=0.8, word_rand=0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prelude: running in notebook for testing only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Special tokens {'unk_token': 100, 'sep_token': 102, 'pad_token': 0, 'cls_token': 101, 'mask_token': 103}\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Loading data from ./demo_data/binarized_text.train.bert-base-uncased.pickle\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Loading token counts from ./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle (already pre-computed)\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Splitting 0 too long sequences.\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Remove 542 too short (<=11 tokens) sequences.\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Remove 0 sequences with a high level of unknown tokens (50%).\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Preparing causal batch.\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  458 sequences\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Data loader created.\n",
      "11/15/2021 11:58:00 - INFO - utils - PID: 51231 -  Loading student config from ./training_configs/distilbert-base-uncased.json\n",
      "11/15/2021 11:58:02 - INFO - utils - PID: 51231 -  Student loaded.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Teacher loaded from bert-base-uncased.\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Initializing Distiller\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Neuron Mapping: {'interchange_variable_mappings': [{'teacher_variable_names': ['$L:[0:4]$H:[0:12]$[0:64]'], 'student_variable_names': ['$L:0$H:[0:12]$[0:64]']}]}\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Using [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499, 503, 507, 511, inf] as bins for aspect lengths quantization\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Count of instances per bin: [54 28 12  7  3  1  1  3  4  3  2  3  4  2  9  6  3  5 13 11  9 11  4  6\n",
      "  5  9  7  8 12  5  7  2  5  9  5  9 11  7  8  8  3  2  3  5  7  6  6  3\n",
      "  7  7  6  2  1  2  8  4  5  3  1  3  4  3  6  3  1  1  2  1  2  2  2  4\n",
      "  1  1  4  1  1  2  2  1  1  1  1  3  1  1  1]\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Using MLM loss for LM step.\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  --- Initializing model optimizer\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  ------ Number of trainable parameters (student): 66592314\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  ------ Number of parameters (student): 66985530\n",
      "11/15/2021 11:58:05 - INFO - utils - PID: 51231 -  Distiller initialization done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Training\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite dump_path if it already exists.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dump_path\", type=str, help=\"The output directory (log, checkpoints, parameters, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_file\",\n",
    "        type=str,\n",
    "        help=\"The binarized file (tokenized + tokens_to_ids) and grouped by sequence.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        type=str,\n",
    "        help=\"You need to give some cache dir.\",\n",
    "        default=\"./distill_cache/\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--student_type\",\n",
    "        type=str,\n",
    "        choices=[\"distilbert\", \"roberta\", \"gpt2\"],\n",
    "        help=\"The student type (DistilBERT, RoBERTa).\",\n",
    "    )\n",
    "    parser.add_argument(\"--student_config\", type=str, help=\"Path to the student configuration.\")\n",
    "    parser.add_argument(\n",
    "        \"--student_pretrained_weights\", default=None, type=str, help=\"Load student initialization checkpoint.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--teacher_type\", choices=[\"bert\", \"roberta\", \"gpt2\"], help=\"Teacher type (BERT, RoBERTa).\"\n",
    "    )\n",
    "    parser.add_argument(\"--teacher_name\", type=str, help=\"The teacher model.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--neuron_mapping\",\n",
    "        type=str,\n",
    "        help=\"Predefined neuron mapping for the interchange experiment.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--temperature\", default=2.0, type=float, help=\"Temperature for the softmax temperature.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_ce\", default=0.5, type=float, help=\"Linear weight for the distillation loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_mlm\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=\"Linear weight for the MLM loss. Must be >=0. Should be used in coonjunction with `mlm` flag.\",\n",
    "    )\n",
    "    parser.add_argument(\"--alpha_clm\", default=0.5, type=float, help=\"Linear weight for the CLM loss. Must be >=0.\")\n",
    "    parser.add_argument(\"--alpha_mse\", default=0.0, type=float, help=\"Linear weight of the MSE loss. Must be >=0.\")\n",
    "    parser.add_argument(\n",
    "        \"--alpha_cos\", default=0.0, type=float, help=\"Linear weight of the cosine embedding loss. Must be >=0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_causal\", default=0.0, type=float, help=\"Linear weight of the causal distillation loss. Must be >=0.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--mlm\", action=\"store_true\", help=\"The LM step: MLM or CLM. If `mlm` is True, the MLM is used over CLM.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mlm_mask_prop\",\n",
    "        default=0.15,\n",
    "        type=float,\n",
    "        help=\"Proportion of tokens for which we need to make a prediction.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--interchange_mlm\", action=\"store_true\", help=\"Whehter to follow mlm to select token positions to do interchange.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--interchange_prop\",\n",
    "        default=0.3,\n",
    "        type=float,\n",
    "        help=\"Ratio of tokens to mask for interchange interventions. 1.0 means interchange all.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--include_crossway\", default=False, action=\"store_true\", help=\"Whether to include crossway losses.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--parallel_crossway\", default=False, action=\"store_true\", help=\"Whether to calculate cross losses in a single step.\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--word_mask\", default=0.8, type=float, help=\"Proportion of tokens to mask out.\")\n",
    "    parser.add_argument(\"--word_keep\", default=0.1, type=float, help=\"Proportion of tokens to keep.\")\n",
    "    parser.add_argument(\"--word_rand\", default=0.1, type=float, help=\"Proportion of tokens to randomly replace.\")\n",
    "    parser.add_argument(\n",
    "        \"--mlm_smoothing\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "        help=\"Smoothing parameter to emphasize more rare tokens (see XLM, similar to word2vec).\",\n",
    "    )\n",
    "    parser.add_argument(\"--token_counts\", type=str, help=\"The token counts in the data_file for MLM.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--restrict_ce_to_mask\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If true, compute the distilation loss only the [MLM] prediction distribution.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_pos_embs\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze positional embeddings during distillation. For student_type in ['roberta', 'gpt2'] only.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freeze_token_type_embds\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Freeze token type embeddings during distillation if existent. For student_type in ['roberta'] only.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--n_epoch\", type=int, default=3, help=\"Number of pass on the whole dataset.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=5, help=\"Batch size (for each process).\")\n",
    "    parser.add_argument(\n",
    "        \"--group_by_size\",\n",
    "        action=\"store_false\",\n",
    "        help=\"If true, group sequences that have similar length into the same batch. Default is true.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Gradient accumulation for larger training batches.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_prop\", default=0.05, type=float, help=\"Linear warmup proportion.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-4, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=5.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--initializer_range\", default=0.02, type=float, help=\"Random initialization range.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--n_gpu\", type=int, default=1, help=\"Number of GPUs in the node.\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Distributed training - Local rank\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=56, help=\"Random seed\")\n",
    "\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=500, help=\"Tensorboard logging interval.\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=4000, help=\"Checkpoint interval.\")\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--is_wandb\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If true, we will log everything to wandb that is logged in currently.\",\n",
    "    )\n",
    "    parser.add_argument(\"--run_name\", type=str, help=\"Name of this run.\")\n",
    "    \n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        parser.set_defaults(\n",
    "            # Exp management:\n",
    "            student_type=\"distilbert\",\n",
    "            teacher_type=\"bert\",\n",
    "            mlm=True,\n",
    "            alpha_ce=0.25,\n",
    "            alpha_mlm=0.25,\n",
    "            alpha_cos=0.25,\n",
    "            alpha_clm=0.0,\n",
    "            alpha_causal=0.25,\n",
    "            token_counts=\"./demo_data/binarized_text.train.token_counts.bert-base-uncased.pickle\",\n",
    "            student_config=\"./training_configs/distilbert-base-uncased.json\",\n",
    "            dump_path=\"./arxiv_results/\",\n",
    "            teacher_name=\"bert-base-uncased\",\n",
    "            force=True,\n",
    "            data_file=\"./demo_data/binarized_text.train.bert-base-uncased.pickle\",\n",
    "            n_gpu=0,\n",
    "            is_wandb=False,\n",
    "            log_interval=10,\n",
    "            neuron_mapping=\"./training_configs/single_multilayer.nm\",\n",
    "            local_rank=-1,\n",
    "            interchange_prop=0.3,\n",
    "            batch_size=5,\n",
    "            gradient_accumulation_steps=50,\n",
    "            include_crossway=False,\n",
    "            parallel_crossway=False,\n",
    "        )\n",
    "        print(\"Prelude: running in notebook for testing only.\")\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        print(\"Prelude: running with command line.\")\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    # config the runname here and overwrite.\n",
    "    data_name = args.data_file.split(\"/\")[-2]\n",
    "    neuron_mapping = args.neuron_mapping.split(\"/\")[-1].split(\".\")[0]\n",
    "    run_name = f\"s_{args.student_type}_t_{args.teacher_type}_data_{data_name}_seed_{args.seed}_mlm_{args.mlm}_ce_{args.alpha_ce}_mlm_{args.alpha_mlm}_cos_{args.alpha_cos}_causal_{args.alpha_causal}_nm_{neuron_mapping}_crossway_{args.include_crossway}\"\n",
    "    args.run_name = run_name\n",
    "    args.dump_path = os.path.join(args.dump_path, args.run_name)\n",
    "    sanity_checks(args)\n",
    "    # for arXiv, we enforce the following settings.\n",
    "    assert not args.include_crossway\n",
    "    assert not args.parallel_crossway\n",
    "    \n",
    "    distiller = prepare_distiller(args)\n",
    "    \n",
    "    # distiller.train()\n",
    "    # logger.info(\"Hey Zen: Let's go get some drinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CausalDistiller at 0x7fd4e1614990>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
